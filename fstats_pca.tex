\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{natbib}

\bibliographystyle{evolution.bst}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normsq}[1]{\left\lVert#1\right\rVert^2}


\newcommand{\MX}{\mathbf{X}} %uncentered data
\newcommand{\MC}{\mathbf{C}} %centering
\newcommand{\MY}{\mathbf{Y}} %centered data
\newcommand{\MF}{\mathbf{F}_2} %F2-distance matrix
\newcommand{\MFT}{\mathbf{F}_3} %F3-distance matrix
\newcommand{\MP}{\mathbf{P}} % PCs
\newcommand{\ML}{\mathbf{L}} % loadings
\newcommand{\MK}{\mathbf{K}} % Kernel
\newcommand{\MSINGULAR}{\mathbf{\Sigma}} % Singular values matrix
\newcommand{\MEIGEN}{\mathbf{\Lambda}} % Eigenvalue matrix


\newcommand{\MEAN}{\boldsymbol{\mu}} % Kernel
\begin{document}
\section{Introduction}
Thanks to the widespread availability of cheap sequencing, genome-wide data sets now frequently incorporate tens of thousands of present-day individuals\cite{gnomad2020}. Furthermore, advances in ancient DNA techniques now allow large data sets. Using this data allows the inference of fine-scale population structure, but translating this wealth of data into meaningful and detailed models of population history is still a major challenge. 

Particularly for the analysis of ancient DNA, two approaches have been proven to be particularly useful: one are global summary analyses, such as Structure \citep{pritchard2000, alexander2009} Principal Component Analysis (PCA) \citep{cavalli-sforza1994, reich2008, novembre2008, mcvean2009} and classical multidimensional scaling (MDS) \cite{fu2016, malaspinas_bammds}. Typically, these methods assume that population structure is \emph{sparse}, so that a low-rank approximation with few underlying ``components'' is sufficient to model population structure See e.g. \cite{engelhardt2010} for a useful perspective how these approaches are related.

Facing a novel data set, PCA or MDS are often the first analyses (beyond quality controls) a researcher performs, in order to obtain insights in the general population structure they are faced with. In order to answer more specific questions and to test specific hypotheses, the $F$-statistic framework of \cite{patterson2012} has been proven particularly powerful (see also \cite{peter2016} for a more gentle introduction). In the $F$-statistic framework, usually only a small number of populations are used at once, to e.g. test for treeness and find closely related populations.

Even though these two approaches are considered in almost every ancient DNA paper, links between the inferences made from them are usually only compared qualitatively. In this paper, our goal is to show that PCA and $F$-statistics are in fact closely related by construction, and use a very similar summary of the data. 


\subsection{Introduction to $F$-statistics}
$F$-statistics have been primarily motivated by trees and admixture graphs \citep{patterson2012, peter2016}, but the calculations hold up in a much wider data space. In particular,  \cite{oteo-garcia2021} provides a thorough introduction to interpreting $F$-statistics in the \emph{data space} $\mathbb{R}^k$. Their work builds much of the foundation of this discussion, by demonstrating analogies to classical geometry. A brief summary of their key results: A population's allele frequencies can be thought of as vector in $\mathbb{R}^k$. Then, $F_2(X_1, X_2) = \normsq{X_1 - X_2}$ is the squared Euclidean distance between the populations with vectors $X_1$ and $X_2$, and $F_4(X_1, X_2 ; X_3, X_4) = \langle X_1 - X_2, X_3 - X_4 \rangle$ is the inner (scalar) product between these two vectors. Here, I will mainly use the $F$-statistic notation, but use the geometric notation where convenient.



	
\section{Relationship of PCA, $F_2$ and Outgroup-$F_3$}

The goal of this section is to give a cursory introduction to $F$-statistics, PCA and MDS, and to define notation. More detailed introductions are given in XXXXX.

\subsection{Introduction to PCA}
	Let us assume we have some genotype data summarized in a matrix $\MX$. Each of the $k$ columns contains the allele-frequencies at a single SNP, and we have $n$ rows; one corresponding to either a population or individual, depending on the desired resolution of the analysis. As a population may be represented by just one (pseudo-)haploid or diploid individual, there is no conceptual difference between these cases, but I will refer to populations as unit for analysis, for simplicity. 
	
	The goal of a PCA is to find a low-dimensional representation of the data that explains most of the variance-structure in the data (see Fig. \ref{fig:pca_explanation} for an intuitive explanation). 
	
	\begin{figure}
		\includegraphics[width=\textwidth]{pca_explanation.png}
		\caption{Basic Idea of PCA from 2D to 1D representation. A: Allele frequencies from different populations (blue dots) at two SNPs. A PCA is performed by centering the data (B), and rotating it (B) such that the first PC explains the majority of variation in the data, and the second PC is orthognal to the first, and explains the residual. A lower-dimensional approximation (in this case 1D) can be achieved by just keeping the first PC (E); which can be translated back to the original data space by inverting the rotation and centering (F).}
		\label{fig:pca_explanation}
	\end{figure}
	
	There are several algorithms that are used to calculate a PCA in practice, the most common one relies on a singular value decomposition. Formally, 
	\begin{equation}
	\MY = \MC\MX = \mathbf{U} \MSINGULAR \mathbf{V}^T = \MP\ML
	\end{equation}
	
	Here, $\MC = \mathbf{I} -\frac{1}{n}\mathbf{1}$ is a centering matrix that subtracts row means, with $\mathbf{I}, \mathbf{1}$ denoting the identity matrix and a matrix of ones, respectively. The orthogonal matrix of principal components $\MP=\mathbf{U}\MSINGULAR$ has size $n \times n$ and is used to reveal population structure. The loadings $\ML=\mathbf{V}^T$ are an orthonormal matrix of size $n \times k$, its rows give the contribution of each SNP to each PC, it is often useful to look for outliers that might be indicative of selection \cite[e.g]{francois2010}.
	
	In many  implementations \citep[e.g]{patterson2006}, SNPs are weighted by the inverse of their standard deviation. As this weighting makes little difference in practice, I will for now assume that SNPs are unweighted, and defer discussion of weighting to a later section.
	
	Equivalently, we obtain the PCs by performing an eigendecomposition of the  covariance matrix denoted as $\MK$:
	 \begin{equation}
 \MK = \MY \MY^T = \mathbf{U}\MEIGEN\mathbf{U}^T = \mathbf{U}\MSINGULAR^2\mathbf{U}^T =\MP\MP^T
	\end{equation} where $\MEIGEN$ is the diagonal matrix with the eigenvalues of $\MK$. 
	This algorithm does not compute the SNP-loadings. However, the $i$-th row of $L$  can be obtained from $\MP$ and the original data, whenever the eigenvalue $\lambda_i \neq 0$:
	\begin{equation}
	\ML_i = \lambda_i^{-1}\MP^T\MC\MX \text{.}
	\end{equation}
	
	
	Let $y_{il}$ denote the genotype of the $i$-th individual at the $l$-th SNP.
	
	\begin{equation}
	y_{il} = x_{il} - \mu_l
	\end{equation}
	where $\mu_l$ is the mean genotype at the $l$-th locus.
	
	
	\subsection{PCA, $F$-statistics and midpoint rooting.}
    The construction of the PCs through the covariance matrix $\MK$ is computationally more intensive than SVD, but it yields a simple connection to $F_3$-statistics.

	Notice that the entries of the covariance matrix $\MK$ are
	\begin{equation}
	k_{ij} = \sum_l y_{il} y_{jl} = \sum_l (x_{il} - \mu_l)(x_{yl} - \mu_l) = F_3(\MEAN; X_i, X_j)\text{,}
	\end{equation}
	where $\MEAN = (\mu_1, \dots \mu_k)$ is the vector of the mean allele frequency at each SNP. One interpretation is that $\MEAN$ denotes a ``pivot''-population whose allele frequency vector is the sample mean of the allele frequency space. By definition, it lies at the origin of the PC-space, and performing a PCA is equal to a rotation around this ``pivot''-population.
	
	
	
	One implication of this is that $\MEAN$ strongly depends on sample composition. If we add many closely related populations (or, in an individual based framework, )This yields an (informal) analogy to the midpoint rooting of a tree in phylogenetics \citep[see e.g.][]{felsenstein2004}. In the absence of any further information, a sensible choice for the root of a tree is the point that is furthest away from all the tips.
	
	
	
	Thus, we can also think of a PCA as performing a decomposition of an Outgroup-$F_3$-matrix, where we set the output to the mean of all marker allele frequencies. This is good practice if we do not have any outgroups available. However, we often also have outgroup data; for example when studying early European variation, Africans are sometimes a suitable outgroup. In this case, it would be consequential to perform an eigendecomposition of a true outgroup $F_3$-matrix.
	
	\paragraph{note}
	This could yield an (informal) analogy to the midpoint rooting of a tree in phylogenetics \citep[see e.g.][]{felsenstein2004}. In the absence of further information, a sensible choice for the root of a tree is the point that is furthest away from all the tips. However,  if we do have better info (i.e. an outgroup), that could yield more sensible results. A caveat is that the eigencomposition of a non-centered matrix does not appear to give the desired properties -- would need to dig substantially deeper into how pca maximises variance explains to come up with better algo.

\subsection{PCA vs metric MDS on $F$-statistics}	
	
This is also a useful way to establish how we can obtain $\MP$ from $\MF$ directly: Note that the row and column means of $\MK$ are zero:
\begin{equation*}
\sum_i k_{ij}= \sum_i\sum_l (x_{il}-\mu_l)(x_{jl} - \mu_l)= \sum_l(x_{jl} - \mu_l)\left[\sum_i x_{il} -\mu_l\right] = 0 \text{.}
\end{equation*}

Since $F_2(X_1, X_2) = F_2(X_1, \mu) + F_2(X_2, \mu) - 2 F_3(\mu; X_1, X_2) \\= \normsq{X_1-\mu} + \normsq{X_2 - \mu} - 2\langle X_1 - \mu, X_2 - \mu \rangle$

(not sure if geometry notation would be easier here),

\begin{equation}
\MK = \MC\MK\MC = \frac{1}{2}\MC\big[\MF(X_1, \mu) + \MF(X_2, \mu) - \MF(X_1, X_2)\big]\MC=-\frac{1}{2} \MC \MF(X_1, X_2) \MC
\end{equation}

since $\MC\MF(X_1, c)\MC = 0$ for all constant $c$. Thus, by double-centering a matrix of $\MF$-values (multiplied by $-\frac{1}{2}$), we can obtain $\MK$ and hence $\MP$. This is exactly what is done in classical multidimensional-scaling, and this derivation is merely a special case of a well-known method.

However, this result goes even further: consider any $F_3$-matrix, where the ``focal''-population is kept constant:
\begin{align}
\MC\MFT(O; X_1, X_2)\MC &= \frac{1}{2}\MC\big[\MF(X_1, O) + \MF(X_2, O) - \MF(X_1, X_2)\big]\MC\nonumber\\
 &=-\frac{1}{2} \MC \MF(X_1, X_2) \MC 
\end{align}

this shows that if we were to mean-center \emph{any} $F_3$-matrix (a standard step in multidimensional scaling) before decomposition, we retain a PCA.

One important detail here are the diagonals of $\MFT$; above results assumes that $F_3(O, X_i, X_i) = F_2(O, X_i)$. This differs from how MDS has been sometimes applied on ancient DNA-data \citep{fu2016}:
\begin{enumerate}
	\item the off-diagonal entries are $1 - F_3(O; X_1, X_2)$ for some outgroup $O$.
	\item the diagonal is 0.
\end{enumerate}
Thus, this matrix differs from that derived above in that one has been added to all off-diagonal entries; and $F_2(X_1,X_2)$ has been subtracted from the diagonal. We have therefore
\begin{align}
\MFT^{(Fu)} = \mathbf{1} - \MFT + \mathbf{O},
\end{align}
where $\mathbf{1}$ is a matrix of ones and $\mathbf{O}$ is a diagonal matrix with entries
$$o_{ii} = F_2(O, X_i) -1$$.

Centering then yields
\begin{align}
-\MC\MFT^{(Fu)}\MC &= -\MC\mathbf{1}\MC + \MC\MFT\MC - \MC\mathbf{O}\MC\nonumber\\
                  &= -\frac{1}{2}\big[\MC\MF\MC +\MC\mathbf{O}\MC\big]\nonumber\\
                  &= -\frac{1}{2}\big[\MC(\MF + \mathbf{O})\MC\big]
\end{align}

\paragraph{open questions}: How big is the effect of $\mathbf{O}$. Is it as expected cancelling out differences in sampling time? If so, are outgroup-$F$-stats generally preferable to PCA?

%In summary, a PCA and an eigendecomposition of the (negative) $\MF$-matrix will give the same results, and can be thought of as an eigendecomposition of the $\MFT$-matrix of an outgroup $F_3$-matrix with the sample mean as an outgroup.




\subsection{Projection using f-stats}
Suppose we have a sample $U$ we wish to project onto an existing PCA-basis made from $\MX$, and let us assume we can compute $F_2(U, X_i)$ for all $i$. The ``best'' point i
For any particular reference sample, $F_2$ places the point on the hypercircle with equation
\begin{equation}
F_2(U, X_i) = \sum_{k=1}^{n-1} (p_{ik} - u_k)^2,
\end{equation}
where $p_{ik}$ and $u_k$ are the reference and unknown coordinate on the $k$-th component, respectively. It can be shown \cite{gower1968} that the $u_k$ can be found using
\begin{equation}
\mathbf{u} = \frac{1}{2}\MEIGEN^{-1}\MP^T\mathbf{d}
\end{equation}
where $\mathbf{d}$ is a column vector with $$d_i = F_2(X_i, \MEAN) - F_2(X_i, U)$$.

Given a fixed projection, this allows us to also propagate uncertainty on a PCA plot.

\paragraph{Potential Application} Use uncertainty on $F_2/F_3$ to propagate uncertainty on PCA-placement


\section{$F$-stats in PCA-space}
In the previous section, we showed that MDS and PCA are closely related to $F$-statistics, when we consider a matrix of many populations. However, one of the main advantages of the $F$-stats framework is that they can be used for specific hypotheses. Thus, in this section I am exploring how these hypotheses relate to the placement of populations on a PCA plot.

Recall that informally, PCA aims to reveal the axes of major variance. To do that, we find an optimal ``rotation'' of the data, such that these axes can be visualized. 

As shown by e.g. \cite{oteo-garcia2021}, $F$-stats can be thought of as inner products in Euclidean space, and $F_2$ is an (estimated) squared Euclidean distance between two populations in allele frequency space. PCA includes a translation and rotation of data, but a distance is invariant to both these operations. Hence, neither mean-centering, which is a translation nor PCA-rotation will change $F_2$. What this means is that we are free to calculate $F_2$ either on the uncentered data $\MX$, the centered data $\MY$ or the principal components $\MP$. Formally,

\begin{align}
F_2(X_i, X_j) &=&  \sum_{l=1}^L \big( x_{il} -x_{jl}\big)^2 - H_i - H_j &&\nonumber\\ 
 &=& \sum_{l=1}^L \big( (x_{il} - \mu_l) -(x_{jl} -\mu_l)\big)^2 - H_i - H_j  &=& F_2(Y_i, Y_j) \nonumber\\
 &=& \sum_k (P_{ik} - P_{jk})^2 - H_i - H_j &=& F_2(P_i, P_j) \text{,}
\end{align}
where $H_i$ and $H_j$ are the bias-correction terms proposed in \cite{reich2009}. A detailed derivation of this is given in Appendix \ref{appendix:fonpc}.
As $F_3$ and $F_4$ can be written as sums of various $F_2$-terms, analogous relations apply.

\subsection{$F$-stats in 2-dimensional PC-space}
 It is useful to consider the statistics on a PCA plot. The relationships we will discuss formally only hold in the full, $n$-dimensional PCA-space where we consider all principal components. Here, we start by discussing 2-dimensional spaces. This is useful for two reasons: for one, the geometry is simpler and we can think of circles and squares as opposed to hyperspheres and other high-dimensional geometric objects and thus help us build intuition. Second, in many applications it is argued that a 2-dimensional approximation is sufficient to explain the major components of population structure. In this case, the results here will hold under the same approximation assumptions in low-dimensional PCs; if they differ substantially from each other, it is likely that not sufficiently many PCs were considered.


\subsubsection{$F_2$ in PC-space}
The $F_2$-statistic as the squared Euclidean distance is the easiest to understand, it corresponds directly to the squared distance in PCA-space. This matches our intuition that closely related populations (which have low $F_2$) will be close to each other on a PCA-plot.

\subsubsection{$F_3$ and circles}
The $F_3$-statistic becomes more interesting; as outlines above we either think of $F_3$ as ``outgroup''-$F$-stats or as admixture $F$-stats. In the admixture case, we may ask the following question: given two source populations $X_1$, $X_2$, where would admixed populations on a PCA plot lie? From theory, we would expect it to lie between $X_1$ and $X_2$, with the exact location depending on sample sizes \cite{brisbin2012, mcvean2009}. 

Formally, we would reject admixture if $F_3$ is negative, i.e. we are looking for the space
\begin{eqnarray}
2 F_3(X_x; X_1, X_2) &=& 2\langle  X_x - X_1, X_x - X_2 \rangle \nonumber\\
      &=& \normsq{X_x - X_1} + \normsq{X_x - X_2}  - \normsq{X_1 - X_2} \nonumber\\
      &<&0
\end{eqnarray}
By the Pythagorean theorem, $F_3 = 0 $ iff $X_1, X_2$ and $X_x$ form a right-angled triangle. Hence, the region where $F_3$ is zero is the circle with diameter through $X_1$ and $X_2$. If $X_x$ lies inside this circle, the angle is obtuse and $F_3$ is negative, otherwise it will be positive. Similarly, if we fix $X_1$ and $X_2$ and ask where on a 2D-PCA-plot $X_2$ would lie, this space is defined by all the points for which the angle between $X_1 X_x$ and $X_2 X_x$ is obtuse.

This highlights a potential identifiability issue with $F_3$: Since all values of $F_3$ that result in the same projection will give the same value; and multiple admixture events may result in the same $F_3$-value.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{dummy_pca.png}
	\caption{\textbf{Geometric representation of $F$-statistics on 2D-PCA-plot.} A: $F_2$ represents the squared Euclidean distance between two points in PC-space. B: Admixture-$F_3(X_x; X_1, X_2)$ is negative if $X_x$ lies in the circle specified by the diameter $X_2-X_1$}. C: $F_3(X_x; X_1, X_2)$ is negative given $X_1, X_x$ if $X_2$ is in the gray space.  D: Outgroup-$F_3$ reflects the projection of $X_2 - X_O$ on $X_1 - X_O$. E: $F_4$ is the projection of $X_3 - X_4$ on $X_1-X_2$. F: If $X_x$ is admixed between $X_1$ and $X_2$, the admixture proportions will be projected.
\end{figure}

\subsubsection{$F_4$ and right angles}
The inner-product-interpretation of $F_4$ is similar to that of $F_3$, with the change that the two vectors we consider do not involve the same population. However, a finding of $F_4(X_1, X_2; X_3, X_4) = \langle X_1 - X_2, X_3 - X4 \rangle = 0$ similarly implies that the two vectors are orthogonal, and a non-zero value reflects the projection of one vector on the other.

\subsubsection{$F_4$-ratio}
\begin{eqnarray}
\frac{F_4(X_I, X_O; X_X, X_1)}{F_4(X_I, X_O; X_2, X_1)} &=& \frac{\norm{X_I-X_O}\norm{X_X-X_1}\cos(\alpha)}{\norm{X_I-X_O}\norm{X_2-X_1}\cos(\beta)}\nonumber\\
&=&\frac{\norm{X_X-X_1}\cos(\alpha)}{\norm{X_2-X_1}\cos(\beta)}\nonumber\\
&=& \frac{\norm{X_X' - X_1'}}{\norm{X_2' - X_1'}}
\end{eqnarray}
where $\alpha$ and $\beta$ are the angles between vectors, and $X_i'$ is the projection of $X_i$ on $X_I-X_O$.

Conjecture: Thus, we are measuring the distances between the admixing populations on the projected on the axis between $X_I$ and $X_O$. This ought to be valid only if $\langle X_1 - X_1', X_2 - X_2' \rangle$ are orthogonal to each other, and to $X_OX_I$, i.e.
$F_4(X_1, X_1', X_2, X_2') = 0$
 
	
\subsection{spectral analysis of admixture statistics}
\begin{enumerate}
	\item split F-stats by PCA basis vector
	\item same F-stat value may arise with different contribution from different PCs, should hint at distinct admixture events
	\item can use clustering to infer shared history?
	\item decomposition of admixture-F3?
%	\item rank of PCA vs qpAdm
\end{enumerate}

\section{Trees and admixture graphs in PCA-space}
\subsection{Trees}
Evolutionary trees are fundamental in phylogenetic analyses, as they, on a large, scale, approximate how taxa diversify. Within a species, applying trees is also very common, but more problematic as populations frequently do not evolve as discrete lineages; instead, they admix and diversify as much more continuous processes. This is largely due to the time-scales involved, speciation events that give rise to trees might often be similarly messy, but from a distance of millions of years these issues might disappear. 

Thus, when estimating trees from population genetic data, we must be very careful about whether the data is actually consistent with a tree, or belongs to some wider class of model.


Trees can be thought of as a collection of orthogonal dimensions; as drift on each branch is independent from every other branch. Thus, each sample is only 
\begin{enumerate}
	\item Trees
	\item Admixture Graphs
	\item Treelets
	\item simple trees, admixture graph
\end{enumerate}




\section{other orthogonal bases}
The most general ``model''-space for (centered) SNP-data $\MY$ is $\mathbb{R}^k$, where we allow each SNP to be in its own dimension, and treat all dimensions as independent. However, since in most analyses the number of samples $n \ll k$, we can place all SNPs in a $n$-dimensional subspace $\mathbb{R}^n$. (Could be restricted further to $[0,1]^k$, but that does not appear to add much).
If the data were normally distributed, $\MK$ has a $n$-dimensional Wishart-distribution with $k$ degrees of freedom. Since SNP are neither normal nor independent, the degrees of freedom might be considerably lower but we might still end up with something normally distributed.






	
\section{Technical considerations}
	\subsection{SNP weighting}
	It is clear that weighting SNP will have some effect on the resulting PCAs. Upweighting rare variants e.g. will emphasis recent events, as rare variance in the sample are more likely to be recent.
	
	
	\subsection{Missing data}
	
	\subsection{What is a dimension?}
	A single population at a particular point in time can be thought of as a single point in allele-frequency space, given by it's $p$-dimensional locus of allele frequencies in that population. If this population evolves for some time in isolation, allele frequencies will change due to genetic drift; i.e. the population evolves along a single tree branch in the interpretation of \cite{patterson2012}. If we now add a second population, it will behave exactly the same, and the drift in the second population will be uncorrelated to the first, i.e. it evolves in a second dimension. Thus, if we have two populations that descend from the same ancestral population in isolation, they can be thought of as evolving along orthognal dimensions from the same point. This argument is at the foundation of F-statistics.
	
	
	\section{outtakes}
	PCA from $\MX$
	\begin{equation}
	\MK = \MY \MY^T = \MC\MX\MX^T \MC = \MP\MP^T
	\end{equation}

\appendix
\section{Derivation}\label{appendix:fonpc}
\begin{eqnarray}
F_2(X_i, X_j) &=& \sum_{l=1}^L \big( (x_{il} - \mu_l) -(x_{jl} -\mu_l)\big)^2 = F_2(Y_i, Y_j)\nonumber\\
&=& \sum_{l=1}^L \big( \sum_k L_{kl}P_{ik} - \sum_kL_{kl}P_{kj}\big)^2\nonumber\\
&=& \sum_{l=1}^L \left( \sum_k L_{kl} (P_{ik} -P_{jk}) \right)^2\nonumber\\
&=& \sum_{l=1}^L \left( \sum_k L_{kl}^2 (P_{ik} -P_{jk})^2 + 2\sum_{k\neq k'} L_{kl}L_{k'l}(P_{ik} - P_{jk'})^2 \right)\nonumber\\
&=& \sum_k \underbrace{\left(\sum_{l=1}^L L_{kl}^2\right)}_1 (P_{ik} -P_{jk})^2 + \sum_{k\neq k'}\underbrace{\left(\sum_{l=1}^L L_{kl}L_{k'l}\right)}_{0} (P_{ik} - P_{jk'})^2\nonumber\\
&=& \sum_k (P_{ik} - P_{jk})^2
\end{eqnarray}

In summary, the first row shows that $F_2$ on the centered data will give the same results (as distances are invariant to translations), in the second row we apply the PC-decomposition. The third row is obtained from factoring out $L_{lk}$. Row four is obtained by multiplying out the sum inside the square term for a particular $l$. We have $k$ terms when for $\binom{k}{2}$ terms for different $k$'s.  Row five is obtained by expanding the outer sum and grouping terms by $k$.The final line is obtained by recognizing that $\ML$ is an orthonormal basis; where dot products of different vectors have lengths zero.

Note that if we estimate $F_2$, unbiased estimators are obtained by subtracting the population-heterozygosities $H_i, H_j$ from the statistic. As these are scalars, they do not change above calculation.
\bibliography{main}

\end{document}
