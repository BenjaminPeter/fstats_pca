\documentclass[12pt,fullpage, a4paper]{article}
\usepackage[
margin=1.5cm,
includefoot,
footskip=30pt,
]{geometry}
\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{lineno}
\usepackage{url}

%\usepackage[authoryear, comma]{natbib}
%\bibliographystyle{plainnat}
%\bibliographystyle{mbe}
\usepackage[style=authoryear,
			natbib=true,
			backend=biber,
			maxbibnames=10,
			maxcitenames=2,
			uniquename=false,
			uniquelist=false,
			isbn=false]{biblatex}
\addbibresource{main.bib}
%biblatex stuff
\DeclareFieldFormat[article]{citetitle}{#1}
\DeclareFieldFormat[article]{title}{#1} 
\renewbibmacro{in:}{}
\DeclareFieldFormat{pages}{#1}
\renewcommand{\labelnamepunct}{\addcolon\space}
\AtEveryBibitem{\clearlist{language}}
\AtEveryBibitem{\clearfield{month}}
\AtEveryCitekey{\clearfield{month}}
\AtEveryBibitem{\ifentrytype{article}{
		\clearfield{url}
		\clearfield{urlyear}
	    \clearfield{language}}{}}
\AtEveryBibitem{\ifentrytype{book}{
		\clearfield{url}
		\clearfield{urlyear}
		\clearfield{language}}{}}
	
	\renewbibmacro*{volume+number+eid}{%
		\printfield{volume}%
		%  \setunit*{\adddot}% DELETED
		\setunit*{\addnbspace}% NEW (optional); there's also \addnbthinspace
		\printfield{number}%
		\setunit{\addcomma\space}%
		\printfield{eid}}
	\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}


\linenumbers



\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normsq}[1]{\left\lVert#1\right\rVert^2}
\newcommand{\MX}{\mathbf{X}} %uncentered data
\newcommand{\MC}{\mathbf{C}} %centering
\newcommand{\MY}{\mathbf{Y}} %centered data
\newcommand{\MF}{\mathbf{F}_2} %F2-distance matrix
\newcommand{\MFT}{\mathbf{F}_3} %F3-distance matrix
\newcommand{\MP}{\mathbf{P}} % PCs
\newcommand{\ML}{\mathbf{L}} % loadings
\newcommand{\MK}{\mathbf{K}} % Kernel
\newcommand{\MSINGULAR}{\mathbf{\Sigma}} % Singular values matrix
\newcommand{\MEIGEN}{\mathbf{\Lambda}} % Eigenvalue matrix
\newcommand{\MEAN}{\boldsymbol{\mu}} % Kernel
\newcommand{\vectorproj}[2][]{\textit{proj}_{#1}#2}


\title{Modelling complex population structure using $F$-statistics and Principal Component Analysis}
\author{Benjamin M Peter}
\begin{document}
	\maketitle
\begin{abstract}
Principal Component Analysis (PCA) and $F$-statistics \textit{sensu} Patterson are two of the most widely used population genetic tools to analyze human genetic variation. Here, I show that these two methods are closely related, and I derive explicit connections between the two approaches. I show that $F$-statistics have a simple geometrical interpretation in the context of PCA, and that orthogonal projections are the key concept to establish this link. I show that admixture corresponds to I illustrate my results on two examples, one of local, and one of global human diversity. In both examples, I find that the first few PCs are sufficient to approximate most $F$-statistics, and thus PCA-plots are effective to predict $F$-statistics. Based on these results, I develop novel visualizations that allow for investigating specific hypotheses, checking the assumptions of more sophisticated models. My results extend $F$-statistics to non-discrete populations, moving towards more complete descriptions of human genetic variation. 
\end{abstract}
\section{Introduction}
As in most species, the genetic diversity of human populations has been influenced by our history and environment over the last several hundred thousand years \citep[e.g][]{cavalli-sforza1994,marciniak2017, reich2018a, nielsen2017, witt2022}. In turn, an important goal of population genetics is to use observed patterns of variation to  investigate and reconstruct the demographic and evolutionary history of our species \citep{schraiber2015, orlando2021}. 

The complicated genetic structure observed in present-day human populations \citep{the2015, mallick2016} is caused by the interplay of demographic and evolutionary processes with both discrete and continuous components \citep{pritchard2000, rosenberg2002a, serre2004, rosenberg2005, bradburd2018, reich2018a, peter2020a, gopalan2022}.
In particular, populations are expected to slowly differentiate if they are isolated from each other \citep{wahlund1928,cavalli-sforza1975}. In humans, this may be caused because continental-scale geographic distances limit migration, causing a pattern known as isolation-by-distance \citep{slatkin1985}. However, these patterns are usually not uniform, but shaped by geography, particularly barriers  to migration such as mountain ranges, oceans or deserts \citep{cavalli-sforza1994, barbujani1990, rosenberg2005, bradburd2013, peter2020a}. In addition,  major historical population movements such as the out-of-Africa, Austronesian or Bantu expansions lead to more gradual patterns of genetic diversity over space \citep{cavalli-sforza1994, ramachandran2005, novembre2008, stoneking2016, racimo2020}. Local migration between neighboring populations will reduce differentiation, and long-distance migrations \citep{alves2016}, and secondary contact between diverged populations, such as Neandertals and modern humans \citep{green2010} may lead to locally increased diversity \citep{boca2020}.

Particularly for large and heterogeneous data sets, disentangling all these processes is challenging, and we cannot expect to devise a single model catching both broad strokes and minute details of human history. A commonly used analysis paradigm is thus to combine tools based on different sets of assumptions, each emphasizing particular aspects of the data.

A typical analysis starts with data-driven, exploratory methods that summarize data making minimal assumptions \citep[e.g.][]{schraiber2015}. Examples are population trees \citep{cavalli-sforza1967, felsenstein1973, cavalli-sforza1975}, Principal Component Analysis \citep[PCA, ][]{cavalli-sforza1994, patterson2006}) structure-like models \citep{pritchard2000, alexander2009} or multidimensional scaling \citep[MDS][]{lessa1990}. These methods are limited in their ability to estimate biologically meaningful parameters, but provide useful summaries and visualizations. These analyses are then complemented with methods based on explicit demographic models, which are used to estimate parameters or test hypotheses \citep{gutenkunst2009, excoffier2013, kamm2015}. 

When the number of populations exceeds a few dozen, even codifying reasonable population models can be prohibitively difficult. One approach is to pick a small set of ``representative'' samples, and restrict modeling to this subset \citep[e.g.][]{gravel2011, harney2021}. However, this has the drawback that a large proportion of the available data remains unused. An increasingly popular alternative approach, particularly in the analysis of human ancient DNA, is therefore to focus on the relationship between two, three or four populations, commonly using $F$-statistics \textit{sensu} Patterson \citep{reich2009, patterson2012, peter2016}. Formal definition will be given in the Theory section; but an informal motivation starts with the null model that populations are related as a tree, in which each $F$-statistic measures the length of a particular set of branches. \citep[Figure \ref{fig:geom};][]{semple2003, peter2016}.

In most applications, $F$-statistics are estimated from data, and then used as tests of treeness. In particular, under the assumption of a tree, $F_3$ is restricted to be non-negative, and many $F_4$-statistics will be zero \citep{semple2003, patterson2012}, and data that violates these constraints is incompatible with a tree-like relationship between populations. The canonical alternative model is an admixture graph (or phylogenetic network) \citep{patterson2012, huson2010}, which is a tree which allows for additional edges reflecting gene flow (Figure \ref{fig:admix}A). However, admixture graphs are not the only plausible alternative model, and expected $F$-statistics can be calculated for a wide range of population genetic demographic models \citep{peter2016}.

\paragraph{F-statistics and PCA}
The practical issue addressed in this study is how $F$-statistics can be compared with PCA, one of the most widely used data-driven modeling  techniques. One way PCA can be motivated is as generating a low-dimensional representation of the data, with each dimension (called principal component, PC) retaining a maximum of the variance present in the data. To understand population structure, the use of PCA has been pioneered by \cite{cavalli-sforza1964}, who used allele-frequency data at a population level to visualize genetic diversity \citep{cavalli-sforza1994}. Currently, PCA is most commonly performed on individual-level genotype data \citep[e.g.][]{patterson2006, novembre2008}, making use of the hundreds of thousands of loci available in most genome-scale data sets. 
%This is most commonly done in early, exploratory phases of a study \citep{schraiber2015}, as PCA can be used to e.g. remove outliers that do not cluster with their population (due to technical or biological reasons), but it remains an important part of many analyses. 
The PCA-decomposition has been studied for a number of explicit population genetic models including trees \citep{cavalli-sforza1975}, spatially continuous structure \citep{novembre2008a}, the coalescent \citep{mcvean2009} and discrete population models \citep{francois2021}. Here, in order to link PCA to $F$-statistics, I interpret both of them geometrically in \emph{allele frequency space}, i.e. as functions of a high-dimensional Euclidean space. For $F$-statistics, this interpretation was recently developed by \cite{oteo-garcia2021}, and for PCA it follows naturally from the interpretation of approximating a high-dimensional space with a low-dimensional one.

In the next section, I will formally derive the connection between $F$-statistics and PCA, and show how $F$-statistics can be interpreted geometrically, with a particular emphasis on two-dimensional PCA plots. In the Results section, I will then discuss how some of the most common applications of $F$-statistics manifest themselves on a PCA, and illustrate them on two example data sets, before ending with a discussion.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{figures/fstats_pca_vs_tree.pdf}
	\caption{\textbf{Representation of $F$-statistics on trees and 2D-PCA-plots.} The schematics show four populations and their representation using an (arbitrarily rooted) tree (top row) or a 2D-PCA plot (bottom row). A: $F_2$ represents the (squared) Euclidean distance between two tree leafs, and in PC-space. B: $F_3(X_1; X_3, X_4)$ corresponds to the external branch from $X_1$ to the internal node joining the populations, and is proportional to the orthogonal projection of $X_1 - X_3$ onto $X_1$-$X_4$. C: $F_4(X_1, X_4; X_2, X_3)$ corresponds to the internal branch in the tree, or the orthogonal projection of $X_2 - X_3$ on $X_1 - X4$. D: $F_4(X_1, X_2; X_3, X_4)$ The two paths from $X_1$ to $X_2$ and $X_3$ and $X_4$ are non-overlapping in the tree, which corresponds to orthogonal vectors in PCA-space.}
	\label{fig:geom}
\end{figure}	
	
\section{Theory}
In this section, I will introduce the mathematics and notations for $F$-statistics and PCA. A comprehensive  treatise on PCA is given by e.g. \cite{jolliffe2013}, a useful primer on the mathematics is \cite{pachter2014}, and a helpful guide to interpretation is \cite{cavalli-sforza1994}. Readers unfamiliar with $F$-statistics may find \cite{patterson2012}, \cite{peter2016} or \cite{oteo-garcia2021} helpful.

\subsection{Formal Definition of $F$-statistics}
Let us assume we have a set of populations for which we have SNP allele frequency data from $S$ biallelic loci. Let $x_{il}$ denote the frequency of an arbitrary allele at the $l$-th SNP in the $i$-th population; and let $X_i = (x_{i1}, x_{i2}, \dots x_{iS})$  be a vector collecting all allele frequencies for population $i$. As $X_i$ will be the only data summary considered here for population $i$, I make no distinction between the population and the allele frequency vector used to represent it.

The three $F$-statistics are defined as
\begin{subequations}
	\begin{align}
	F_2(X_1, X_2) =& \frac{1}{S}\sum_{l=1}^S(x_{1l} - x_{2l})^2
	\\
	F_3(X_1; X_2, X_3) =& \frac{1}{S}\sum_{l=1}^S(x_{1l} - x_{2l})(x_{1l} - x_{3l}) \\	
	F_4(X_1, X_2; X_3, X_4) =& \frac{1}{S}\sum_{l=1}^S(x_{1l} - x_{2l})(x_{3l} - x_{4l}) 	\text{,}
	\end{align}
\end{subequations}
The normalization by the number of SNPs $S$ is assumed to be the same for all calculations and is thus omitted subsequently. Both $F_3$ and $F_4$ can be written as sums of $F_2$-statistics:
\begin{subequations}
	\begin{align}
	2F_3(X_1; X_2, X_3) &=  F_2(X_1, X_2) + F_2(X_1, X_3) - F_2(X_2, X_3)\label{eq:f3fromf2}\\
	2F_4(X_1, X_2; X_3, X_4) &= F_2(X_1, X_3) + F_2(X_2, X_4) - F_2(X_1,X_4) - F_2(X_2, X_3)\label{eq:f4fromf2}
	\end{align}
\end{subequations}

Commonly, a distinction is made between statistics estimated from data (denoted with lowercase-$f$), and theoretical quantities (defined in eq. 1). I do not make this distinction, but will explicitly mention when I analyze statistics calculated from data.
 
$F$-statistics have been primarily motivated in the context of trees and admixture graphs \citep{patterson2012}. In a tree, the squared Euclidean distance $F_2(X_1, X_2)$ measures the length of the path between populations $X_1$ and $X_2$ (Figure \ref{fig:geom}A); $F_3$ represents the length of an external branch (Figure \ref{fig:geom}B) and $F_4$ the length of an internal branch, respectively (Figure \ref{fig:geom}C). The length of each branch can be thought of in units of genetic drift, and is non-negative \citep{patterson2012}. Crucially, this means that $F_4$ will be zero for pairs of populations from non-overlapping clades, which means that the tree lacks the branch corresponding to this statistic (as in Figure \ref{fig:geom}D).
 
Thinking of $F$-statistics as branch lengths is useful for a number of applications, including building multi-population models \citep{patterson2012, lipson2013}, estimating admixture proportions \citep{petr2019, harney2021} and finding the population most closely related to an unknown sample (``Outgroup''-$F_3$-statistic).

%In particular, one common task is to find the population most closely related to an unknown sample $X_U$ \citep{raghavan2014}. One way to do that is using an \emph{outgroup}-$F_3$-statistic $F_3(X_O; X_U, X_i)$, where $X_O$ denotes an outgroup, and the $X_i$ are a panel of populations that are candidates for the closest match. The largest value of $F_3$ indicates the population $X_i$ most closely related to $U$, using the outgroup $O$ to correct for differences in sample times. The intuition is given in Figure \ref{fig:outgroupf3}A; where the outgroup-$F_3$-statistic $F_3(X_O; X_U, X_3)$ is highlighted. It represents the length of the branch from $X_O$ to the common node between the three samples in the statistic, and the closer this node is to $X_U$, the longer the branch and hence the larger the statistic. In contrast to a simple genetic distance, the sample time has no effect: The branch between $X_U$ and $X_2$, would be shorter than to one between $X_U$ and $X_1$, but the path to the shared junction and hence the $F_3$-statistic would be the same. Larger sets of $F_3$ and $F_4$-statistics are also frequently used for complex models, such as  reconstructing admixture graphs \citep{patterson2012, lipson2013} and estimating admixture proportions \citep{petr2019, harney2021}.

Most commonly however, $F_3$ and $F_4$ are used as tests of treeness \citep{patterson2012}: Negative $F_3$-values correspond to a branch with negative genetic drift, which is not allowed under the null assumption of a tree-like population relationship. Similarly if four populations are related as a tree, then at least one of the $F_4$ statistics between the populations will be zero \citep{buneman1974, patterson2012}. 

The most widely considered alternative model is an admixture graph \citep{patterson2012}, an example is given in Figure \ref{fig:admix}A. Here, the (typically unobserved) population $X_y$ is generated by a mixture of individuals from the ancestors of $X_2$ and $X_3$. Over time, genetic drift will change $X_y$ to $X_x$, which is the admixed population we observe. In this case, all $F_4$-statistics involving $X_y$ and both admixture sources will be non-zero, and, in some cases, $F_3(X_y; X_2, X_3)$ will be negative \citep[exact conditions can be found in ][]{peter2016}. 

\subsubsection{Geometric interpretation of $F$-statistics}
An implicit assumption in the development of $F$-statistics is that population lineages are mostly discrete, and that gene flow is rare. Specifically, they interpret the populations $X_i$ as points or vectors in the $S$-dimensional \emph{allele frequency space} $\mathbb{R}^S$. In this case, the $F$-statistics can be thought of as inner (or dot) products, and they showed that all properties and tests related to treeness can be derived in this larger space. In particular the $F$-statistics can be written as
\begin{subequations}
	\begin{align}
	&F_2(X_1, X_2) &=& \frac{1}{S}\sum_{l=1}^S(x_{1l} - x_{2l})^2
	&=& \frac{1}{S}\langle X_1 - X_2, X_1 - X_2 \rangle = \frac{1}{S}\normsq{X_1-  X_2}\\
	&F_3(X_1; X_2, X_3) &=& \frac{1}{S}\sum_{l=1}^S(x_{1l} - x_{2l})(x_{1l} - x_{3l}) &=& \frac{1}{S}\langle X_1 - X_2, X_1 - X_3 \rangle\\	
	&F_4(X_1, X_2; X_3, X_4) &=& \frac{1}{S}\sum_{l=1}^S(x_{1l} - x_{2l})(x_{3l} - x_{4l}) &=& \frac{1}{S}\langle X_1 - X_2, X_3 - X_4 \rangle	\text{,}
	\end{align}
\end{subequations}
where $\norm{\cdot}$ denotes the Euclidean norm and $\langle \cdot, \cdot \rangle$ denotes the dot product. Some elementary properties of the dot product between vectors $a, b, c$ that I will use later are
\begin{subequations}
	\begin{align}
	\langle a, b \rangle &= \sum_i a_ib_i\\
	\langle a, b \rangle &= \norm{a}\norm{b}\cos(\phi)\\
	\langle a, a \rangle &= \normsq{a}\\
	\langle a + c, b \rangle &= \langle a, b \rangle + \langle b, c \rangle,
	\end{align}
\end{subequations}
where $\phi$ is the angle between $a$ and $b$. The inner product is  closely related to the vector projection
\begin{equation}
\vectorproj[b]{a} = \frac{\langle a , b\rangle}{\normsq{b}} b,\label{eq:proj}
\end{equation}
which is a vector colinear to $b$ whose length measures how much vector $a$ points in the direction of $b$. Thinking of $F$-statistics as projections also holds on trees: In e.g. a $F_4(X_1, X_4; X_2, X_3)$-statistic (Figure \ref{fig:geom}C), the internal branch is precisely the intersection of the paths from $X_1$ to $X_4$ and from $X_2$ and $X_3$. On trees, all disjoint paths are independent (i.e. orthogonal) from each other, and thus the external branches vanish under the projection.

The geometric approach of \cite{oteo-garcia2021} assumes a very high-dimensional space, as each SNP in a data set adds a dimension, and data sets have commonly a million or more SNPs. Such high-dimensional spaces are hard to visualize and analyze, however, it has been commonly observed that population structure is quite low-dimensional, and that the first few PCs provide a good approximation of the covariance structure in the data \citep{patterson2006}. Therefore, we may hope that PCA could yield a reasonable approximation of the allele frequency space, and that $F$-statistics as measures of population structure may likewise be well-approximated by the first few PCs.





\subsection{Formal Definition of PCA}
PCA is a common way of summarizing genetic data, and so a large number of variations of PCA exist, e.g. in how SNPs are standardized, how missing data are treated or whether we use individuals or populations as units of analysis \citep{cavalli-sforza1994, patterson2006}. The version of PCA I use here is set up such that the similarities to  $F$-statistics are maximized, and does \emph{not} reflect how PCA is most commonly applied to genome-scale human genetic variation data sets. In particular, I assume that a PCA is performed on unscaled, estimated population allele frequencies, whereas many applications of PCA are based on individual-level sample allele frequency, scaled by the estimated standard deviation of each SNP \citep{patterson2006}. The differences this causes will be addressed in the discussion.

Let us again assume we have allele frequency data as above, but let us now assume we aggregate the allele frequency vectors $X_i$ of many populations in a matrix $\MX$ whose entry $x_{il}$ reflects the allele frequency of the $i$-th population at the $l$-th genotype. If we have $S$ SNPs and $n$ populations, $\MX$ will have dimension $n \times S$. Since the allele frequencies are between zero and one, we can interpret each population $X_i$
of $\MX$ as a point in $\mathbb{R}^S$.
	
PCA allows us to approximate the points in the high-dimensional allele frequency space by a $K$-dimensional subspace of the  data. If all PCs are considered, $K=n-1$, in which case the data are simply rotated. However, the  historical processes that generated genetic variation often result in \emph{low-rank} data \citep{engelhardt2010}, so that $K \ll n$ explains a substantial portion of the variation; for visualization $K=2$ is frequently used. 
		
There are several algorithms that are used to perform PCAs, the most common one is based on singular value decomposition \citep[e.g.][]{jolliffe2013}. In this approach, we first mean-center $\MX$, obtaining a centered matrix $\MY$
	\begin{equation*}
	y_{il} = x_{il} - \mu_l
	\end{equation*}
	where $\mu_l$ is the mean allele frequency at the $l$-th locus.
	
	PCA can then be written as
	
	\begin{equation}
	\MY = \MC\MX = (\mathbf{U} \MSINGULAR) \mathbf{V}^T = \MP\ML\text{,} \label{eq:svd}
	\end{equation}
	
	where $\MC = \mathbf{I} -\frac{1}{n}\mathbf{1}$ is a centering matrix that subtracts row means, with $\mathbf{I}, \mathbf{1}$  the identity matrix and a matrix of ones, respectively. For any matrix $\MY$, we can perform a singular value decomposition $\MY = \mathbf{U}\MSINGULAR\mathbf{V}^T$ which, in the context of PCA, is interpreted as follows: The matrix of principal components $\MP=\mathbf{U}\MSINGULAR$ has size $n \times n$ and contains information about population structure. The SNP loadings $\ML=\mathbf{V}^T$ form an orthonormal basis of size $n \times S$, its rows give the contribution of each SNP to each PC. It is often used to look for outliers, which might be indicative of selection \citep[e.g][]{duforet-frebourg2016}. Alternatively, the PCs can also be obtained from an eigendecomposition of the covariance matrix $\MY\MY^T$. This can be motivated from (\ref{eq:svd}):
	
	\begin{equation}
	\MY\MY^T = \MP\ML\ML^T\MP^T = \MP\MP^T,
	\end{equation}
	since $\ML\ML^T=\mathbf{I}$.

\subsection{Connection between PCA and $F$-statistics}	
\subsubsection{Principal components from $F$-statistics}
PCA, as defined above, and $F$-statistics are closely related. It is a classical result that PCA is equivalent to multidimensional scaling using squared Euclidean distances \citep{gower1966}. Since $F_2$-distances are squared Euclidean, we calculate the pairwise $F_2(X_i, X_j)$ between all $n$ populations, and collect them in a matrix $\MF$. Multidimensional scaling then proceeds by double-centering it, so that its row and column means are zero, and perform an eigendecomposition of the resulting matrix:
\begin{equation}
\MP\MP^T = - \frac{1}{2}\MC\MF\MC \text{.} \label{eq:mds}
\end{equation}
Although we arrive at $\MP$ from a very different angle, as long as we make the same choices about normalization and units of analysis, we will get the exact same results.

\subsubsection{$F$-statistics in PCA-space}
By performing a PCA, we rotate our data to reveal the axes of highest variation. However, the dot product is invariant under rotation, and $F$-statistics can be thought of as dot products \citep{oteo-garcia2021}.  What this means is that we are free to calculate $F_2$ either on the uncentered data $\MX$, the centered data $\MY$ or any other orthogonal basis such as the principal components $\MP$. Formally,

\begin{align}
F_2(X_i, X_j) &=&  \sum_{l=1}^L \big( x_{il} -x_{jl}\big)^2  &&\nonumber\\ 
&=& \sum_{l=1}^L \big( (x_{il} - \mu_l) -(x_{jl} -\mu_l)\big)^2   &=& F_2(Y_i, Y_j) \nonumber\\
&=& \sum_{k=1}^n (p_{ik} - p_{jk})^2  &=& F_2(P_i, P_j) \text{,}\label{eq:fpc}
\end{align}
A derivation of this change-of-basis is given in Appendix \ref{appendix:fonpc}, Equation \ref{eq:changeofbasis}.
As $F_3$ and $F_4$ can be written as sums of $F_2$-terms (Eqs. \ref{eq:f3fromf2}, \ref{eq:f4fromf2}), analogous relations apply. 

In most applications, we do not use all PCs, but instead truncate to the first $K$ PCs, which explain most of the between-population genetic variation.
Thus, 
\begin{align}
F_2(P_i, P_j) = \underbrace{\sum_{k=1}^K(p_{ik} - p_{jk})^2}_{\hat{F_2}^{(K)}(P_i, P_j)} + \underbrace{\sum_{k=K+1}^n(p_{ik} - p_{jk})^2}_{\epsilon^{(K)}(P_i, P_j)} \text{.}
\end{align}
In this notation, $\hat{F_2}^{(K)}$ is the approximation of $F_2$ with only the first $K$ PCs considered, and $\epsilon^{(K)}$ is the corresponding approximation error. I will omit the superscript of $\hat{F_2}$ when the exact number of PCs is not relevant.
If we sum up the squared approximation errors over all pairs of populations in our sample, we obtain 
\begin{equation}
\sum_{i,j} \epsilon^{(K)}(P_i, P_j)^2 = \sum_{i,j} \left(\hat{F_2}^{(K)}(P_i, P_j) - F_2^{(K)}(P_i, P_j)\right)^2 = \normsq{\MF - \hat{\MF}}_F \text{,}
\end{equation}
where the Frobenius-norm $\normsq{\cdot}_F$ of a matrix is defined as the square root of the sum-of-squares of all its elements. This is precisely the function that is minimized in MDS \citep{jolliffe2013}. In that sense, $\hat\MF^{(K)}$ is the optimal low-rank approximation of $\MF$ for any $K$ in that it minimizes the sum of approximation errors of all $F_2$-statistics.

\subsubsection{$F$-statistics and samples projected onto PCA}
One of the easiest ways of dealing with missing data in PCA is to calculate the principal components (equation \ref{eq:svd}) only on a subset of the data with no missingness, and then to \emph{project} the lower quality samples with high missingness onto this PCA. The simplest way to do this is to note that

\begin{equation*}
\MY\ML^T = \MP\ML\ML^T = \MP,
\end{equation*}
and so a new (centered) population $Y_{\text{new}}$ can be projected onto an existing PCA simply by post-multiplying it with $\ML^T$: $$P_{\text{proj}} = Y_{\text{new}}\ML^T;$$ the $k$-th entry of $P_{\text{proj}}$ gives the coordinates of the new sample on the $k$-th PC. However, it is likely that $Y_{\text{new}}$ lies outside the variation of the original samples. In this case, there is a projection error 
$$\normsq{Y_{\text{new}} - P_{\text{proj}}\ML } =  F_2(P_{\text{proj}}\ML, Y_{\text{new}}).$$ If we project with missing data, a similar projection can be used where we remove the rows from $Y_{\text{new}}$ and $\ML$ where data in $Y_{\text{new}}$ is missing, and add a scaling factor \citep{patterson2006}.



%Equivalently, if we have $F_2$-distances from the 
%$$p_{\text{new}} = \frac{1}{2} \MSINGULAR^{-2}P^T d,$$
%where $d_i = \normsq{X_i} - F_2(X_i, X_{\text{new}})$ \citep{gower1968}. In this case, 

Thus, if we compare the $F$-statistic of a projected sample, we have 

\begin{align}
F_2(X_i, X_{\text{new}}) &= F_2(Y_i, Y_{\text{new}})\nonumber\\ 
&= F_2(P_i, P_{\text{proj}}) + F_2(P_{\text{proj}}\ML, Y_{\text{new}})\nonumber\\
&= \hat{F_2}(P_i, P_j) + {\epsilon(P_i, P_j)} + F_2(P_{\text{proj}}\ML, Y_{\text{new}}) \label{eq:approx}.
\end{align}
The second row follows because the projection error and projection are orthogonal to each other. The main implication of equation \ref{eq:approx} is that both truncation and projection introduce some error, and that $ \hat{F_2}(P_i, P_j)$ will be a good approximation to  $F_2(P_i, P_j)$ only if both errors are small.


\section{Material \& Methods}
The theory outlined in the previous section suggests that $F$-statistics have a geometric interpretation in PCA-space, which can be approximated on PCA plots. In the next section I explore this connection in detail, and illustrate it on two sample data sets that I briefly introduce here. Both are based on the analyses by \cite{lazaridis2014}. The data are from the Reich lab compendium data set (v44.3), downloaded from \url{https://reich.hms.harvard.edu/allen-ancient-dna-resource-aadr-downloadable-genotypes-present-day-and-ancient-dna-data}, using data on the ``Human Origins''-SNP set (597,573 SNPs). SNPs with missing data in any population are excluded. The code used to write this paper, create all figures and analyses is  available on \url{https://github.com/BenjaminPeter/fstats_pca}.

\paragraph{``World'' data set}
This data set is a subset of the ``World Foci'' data set of \cite{lazaridis2014}, where I removed samples that are not permitted for free reuse. These populations span the globe and roughly represents global human genetic variation (638 individuals from 33 population) As adjacent sampling locations are often thousands of kilometers apart, I speculate that gene flow between these populations may not be particularly common; and their structure may therefore be well-approximated by an admixture graph. A file with all individuals used and their assigned population is given in \textbf{Supplementary File 1.}

\paragraph{Western Eurasian data set}
This data set of 1,119 individuals from 62 populations contains  present-day individuals from the Eastern Mediterranean, Caucasus and Europe.  \cite{lazaridis2014} used this data set as a basis of comparison for ancient genetic analyses of Western Eurasian individuals, and PCAs based on similar sets of samples have been used in many other ancient DNA studies \citep[e.g.][]{lazaridis2016, haak2015}. Genetic differentiation in this region is low and closely mirrors geography \citep{novembre2008}. I thus speculate that gene flow between these populations is common \citep{ralph2013}, and a discrete model such as a tree or an admixture graph might be a rather poor reflection of this data. A file with all individuals used and their assigned population is given in \textbf{Supplementary File 2.}


\paragraph{Computing $F$-statistics and PCA}
All computations are performed in \texttt{R}. I use \texttt{admixtools 2.0.0} (\url{https://github.com/uqrmaie1/admixtools}) to compute $F$-statistics. To obtain a PC-decomposition, I first calculate all pairwise $F_2$-statistics, and then use equation \ref{eq:mds} and the \texttt{eigen} function to obtain the PCs. The right-hand side matrix of equation \ref{eq:mds} is expected to have non-negative eigenvalues (i.e. $-\MC\MF\MC$ is positive-semidefinite). However when $F_2$-statistics are estimated from data, sampling noise might make some of them  slightly negative, which would lead to imaginary PCs. I avoid this by setting all negative eigenvalues to zero.

\section{Results}



\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{figures/fstats_admixture_pca.pdf}
	\caption{\textbf{Admixture representation on 2D-PCA-plot.} The schematics show four populations and their representation using an admixture graph (A) or a 2D-PCA plot. A: Admixture graph, with population $X_y$ originating as an admixture of $X_2$ (with proportion $1-\alpha$)and $X_3$ (proportion ($\alpha$), with $X_2$ contributing proportion $\alpha$. Subsequent drift (red branch) will change allele frequency to sampled admixture population $X_x$. B: PCA representation of the scenario in A. $X_y$ originates on the segment connecting $X_2$ and $X_3$, and subsequent drift may move it in a random direction. C: Negative region (light gray) for $F_3(X_x; X_2, X_3)$ and for $\hat{F}_3^{(2)}(X_x; X_2, X_3)$ based on two dimensions (dark gray). D: $F_4(X_1, X_x; X_3, X_4)$ will no longer be zero (compare to Figure \ref{fig:geom}D). }
	\label{fig:admix}
\end{figure}


%\subsection{Geometry of $F$-statistics in PC-space}
The transformation from the previous section allows us to consider the geometry of $F$-statistics in PCA-space. The relationships we will discuss formally only hold if we use all PCs. However, the appeal of PCA is that frequently, only a very small number $K \ll n$ of PCS contain most information that is
relevant for population structure, in which case the geometric interpretations become very simple. Thus, throughout the schematic figures, I assume that two PCs are sufficient to characterize population structure. In the data applications I evaluate how deviations of this assumption may manifest themselves in PCA plots.

\subsection{$F_2$ in PC-space}
The $F_2$-statistic is an estimate of the squared allele-frequency distance between two
populations. On a tree (Figure \ref{fig:geom}A) this corresponds to the branch between two populations (Figure \ref{fig:geom}E). In allele-frequency space, it corresponds to the squared Euclidean distance, and thus reflects the intuition that closely related populations  will fall close to each other on a PCA-plot, and have low pairwise $F_2$-statistics. However, since $F_2$ can be written as  a sum of squared (non-negative) terms for each PC (eq. \ref{eq:fpc}), the distance on a PCA-plot will always be an underestimate of the full $F_2$-distance. Thus, PCA might project two populations with high $F_2$-distance very close to each other, which would indicate that these particular PCs are not suitable to  understand and visualize the relationship between these particular populations, and likely more PCs need to be investigated to understand how these populations are related to each other. In converse, populations that are distant on the first few PCs are guaranteed to also have a large $F_2$-distance, since the distance contributed from the omitted PCs cannot be negative.

\begin{figure}[!ht]
	\includegraphics[width=.4\textwidth]{figures/fig_proj.pdf}
	\caption{\textbf{Admixture-$F_3$-statistic and Dimensionality reduction.} A: The gray circle is defined by $X_1$ and $X_2$ and reflects the area for which $F_3$ is negative, i.e. $F_3(X_x; X_1, X_2) < 0 < F_3(X_3; X_1, X_2) \leq F_3(X_4; X_1, X_2)$. B: Projection on one dimension. The full rejection region (light gray bar) is given by the boundaries of the circle in A, and is larger than that predicted from the distance between $X_1$ and $X_2$ (dark grey bar) on PC1 alone. Points like $X_x$ with negative $F_3$ are guaranteed to lie in the light gray (but not dark gray) region. $X_3$ also projects onto the gray bar, even though it is outside the circle and $F_3$ is positive, and it would also lie inside the positive area if projected onto PC2. However, points such as $X_4$ that project outside the rejection region have $F_3>0$, since the associated projection line does not intercept the circle.}
	\label{fig:proj}
\end{figure}


\subsection{When are admixture-$F_3$ statistics negative?}
Consider again the admixture scenario in Figure \ref{fig:admix}A, where population $X_y$ is the result of a mixture of $X_2$ and $X_3$, and subsequent drift changes the allele frequencies of the admixed population from $X_y$ to $X_x$. How is such a scenario displayed on a PCA?  Since the allele frequencies of $X_y$ are a linear combination of $X_2$ and $X_3$, it will lie on the line segment connecting these two populations (Figure \ref{fig:admix}B), at a location predicted by the admixture proportions. Subsequent drift will change the allele frequencies of $X_x$ (to say, $X_y$), and so in general it might fall on a different point on a PCA-plot. An exception occurs when $X_x$ (and no other populations related to $X_x$) are not part of the construction of the PCA, so that $X_x - X_y$ is orthogonal to all PCs, i.e. $$\langle X_x - X_y, X_i - X_j \rangle = \langle X_x - X_y, P_i \rangle = 0$$ for all populations $i,j \leq n$. In this case, $X_x$ and $X_y$ project to the same point, and the location on the PCA  can directly be used to predict the admixture proportions \citep{mcvean2009, brisbin2012, oteo-garcia2021}. However, if either $X_x$, is included in the construction of the PCA, or if some gene flow occurred between $X_x$ and any of the populations used to construct the PCA, $X_x$ and $X_y$ may project on different spots (Figure \ref{fig:admix}B). 

Thus, a natural question to ask is given two source populations $X_2$, $X_3$, can we use PCA to predict which populations might have negative $F_3$-statistics? This condition can be written as  
\begin{eqnarray}
2 F_3(X_x; X_2, X_3) &=& 2\langle  X_x - X_2, X_x - X_3 \rangle \nonumber\\
      &=& \normsq{X_x - X_2} + \normsq{X_x - X_3}  - \normsq{X_2 - X_3} < 0 \label{eq:f3neg}\text{.}
\end{eqnarray}
By the Pythagorean theorem, $F_3 = 0 $ if and only if $X_2, X_3$ and $X_x$ form a right-angled triangle. The associated region where $F_3=0$ is a $n$-sphere (or a circle in two dimensions) with diameter $\overline{X_2X_3}$ (The overline denotes a line segment). $F_3$ is negative when the triangle is obtuse, i.e. $X_x$ could be considered admixed if it lies inside the $n$-ball with diameter $\overline{X_2X_3}$ (Figure \ref{fig:geom}B, Equation \ref{eq:f3sphere}). 

\paragraph{$F_3$ and dimensionality reduction} 
 If we project this $n$-ball on a two-dimensional plot, $\overline{X_2X_3}$ will usually not align with the PCs; thus the ball may be somewhat larger than it appears on the plot. This geometry is perhaps easiest visualized on the example of projecting $F_3(X_x; X_1, X_2)$ from a two-dimensional space onto a single dimension (Figure \ref{fig:proj}). In that example, the distance between $X_1$ and $X_2$ has a substantial contribution from PC2, and so the negative region (light gray) is larger than what would be predicted from just one dimension (dark gray bar), but if $\hat{F}_3 \approx F_3$, the two areas would be very close. Thus, if considering a reduced-dimension PCA-plot, some points (such as $X_3$) may project inside the negative region, but have positive $F_3$ because they are outside the $n$-ball in higher dimensions. The converse interpretation is more strict: if a population lies outside the circle on \emph{any} projection, $F_3$ is guaranteed to be bigger than 0 (see Equation \ref{eq:f3circle} in the Appendix). An intuitive example is given by $X_4$ in Figure \ref{fig:proj}: all points projecting to the same point on Figure \ref{fig:proj}B as $X_4$ lie outside the circle.

\paragraph{Example}
As an example, I visualize the admixture statistic $F_3(X; \text{Sardinian}, \text{Finnish})$, on the first two PCs of the Western Eurasian data set (Figure \ref{fig:weu}A). In this case, the projected $n$-ball (light gray) and circle based on two dimensions (dark gray) have similar sizes. However, several populations that appear inside the circles (e.g. Basque, Canary Islanders) have, in fact, positive $F_3$-values, so they lie outside the $n$-ball. This reveals that the first two PCs do not capture all the genetic variation relevant for  European population structure.  Consequently, approximating $F_3$ by the first two or even ten PCs (Figure \ref{fig:weu}B) only gives a coarse approximation of $F_3$, and from Figure \ref{fig:weu}C we see that many higher PCs contribute to $F_3$ statistics.

However, many populations, particularly from Western Asia and the Caucasus, on the right-hand side of the plot, fall outside the circle. This allows us to immediately conclude that their $F_3$-statistics must be positive; and we should not consider them as a mixture between Sardinians and Fins.


\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{figures/fig_data_europe.pdf}
	\caption{\textbf{PCA and $F$-statistics for the Western Eurasian data set} A: PCA-biplot; the light grey circle denotes the region for which $F_3(X; \text{Sardinian}, \text{Finnish})$ may be negative, the dark circle is based on just the first two PCs. Populations for which $F_3$ is negative are colored in red. B: $F_3$ approximated with two (blue) and ten (red) PCs versus the full spectrum. C: Boxplot of contributions of PCs 1-10 to each $F_3$-statistic. D: Projection angle and correlation interpretation of $F_4(X, \text{Saudi}; \text{Sardinian}, \text{Finnish})$ based on two PCs (green), three PCs (blue) or full data (red). E: Contribution of the first ten PCs to select $F_4$-statistics, with the first three PCs containing the majority of contributions.}
	\label{fig:weu}
\end{figure}


\begin{figure}[!ht]
	\includegraphics[width=.6\textwidth]{figures/outgroup_f3.pdf}
	\caption{\textbf{Outgroup-$F_3$-statistics}. Interpretation of outgroup $F_3$-statistic on a tree (A) and PCA-plot (B).  The red segment represents $F_3(X_O; X_U, X_3)$ and the dashed blue segment reflects $F_3(X_O; X_U, X_1)$ and $F_3(X_O; X_U, X_2)$, which have the same value. 
	}
	\label{fig:outgroupf3}
\end{figure}
\subsection{Outgroup-$F_3$-statistics as projections}
A common application of $F_3$-statistics is, given an unknown sample $X_U$, to find the most closely related population among a reference panel $(X_i)$ \citep{raghavan2014}. This is done  using an \emph{outgroup}-$F_3$-statistic $F_3(X_O; X_U, X_i)$, where $X_O$ is an outgroup. The reason an outgroup is introduced is to account for differences in sample times and additional drift in the reference populations (Figure \ref{fig:outgroupf3}A). The outgroup-$F_3$-statistic $F_3(X_O; X_U, X_3)$ represents the branch length from $X_O$ to the common node between the three samples in the statistic, and the closer this node is to $X_U$, the longer the branch and hence the larger the $F_3$-statistic. 

To make sense of outgroup-$F_3$-statistics in the PCA context, I use the association of $F_3$-statistics to projections (Equation \ref{eq:proj}):
On a PCA-plot, we can visualize this $F_3$-statistic as the projection of the vector $X_i - X_O$ onto $X_U - X_O$: 
$$\vectorproj[X_U - X_O]{X_i - X_O} =F_3(X_O; X_U, X_i) \frac{X_U - X_O}{F_2(X_O; X_U)}.$$

Of the right-hand-side terms, only the $F_3$ term depends on the $X_i$. The fraction can be thought of as a normalizing constant,  so the $F_3$-statistic is proportional to the length of  the projected vector, and they can thus be interpreted similarly. This means, that the outgroup-$F_3$-statistic is largest for whichever $X_i$ projects furthest along the axis from the outgroup to the unknown population; in Figure \ref{fig:outgroupf3} this is $X_3$.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{figures/fig_data_world.pdf}	
	\caption{\textbf{PCA and $F$-statistics for the World data set} A: Visualization of Outgroup-$F_3$-statistic $F_3(\text{Mbuti}; \text{Sardinian}, X)$ on a PCA-biplot. The color of points correspond to the value of the $F_3$ statistic, with brighter yellows indicating higher values, i.e. higher similarity to Sardinians. The $F_3$-projection axis is given by a black line, the projection of populations onto this axis by thin gray lines. In the full allele frequency  space, these projection are orthogonal to the axis. B: Projection along the axis Sardinian-Mbuti (X-axis), and PCA on residual of this projection (PC1 on Y-axis, PC2 as coloring). C: Approximation of $F_3(\text{Mbuti}; \text{Sardinian}, X)$ using the first two (blue) and first ten (red) PCs, respectively. D: Contributions of first ten PCs to all statistics of the form $F_3(\text{Mbuti}; \text{Sardinian}, X)$. E: Contributions of the first ten PCs to select $F_4$-statistics.  
	}	
	\label{fig:world}
\end{figure}


%geometric interpretations of which are, for $F_3$ and $F_4$ given in Figure \ref{fig:geom}F-H. The term in parentheses gives the direction of the resulting vectors, and the the numerator of the right-hand side is the $F$-statistic. The denominator is the distance between the last two arguments, in many applications we compare different $F$-statistics with repeated populations, and so it is useful to think of $F$-statistics as representing (i.e. being proportional to) projections. This yields a simple representation of $F$-statistics on PCA plots that again, is exact when the first two PCs explain all relevant population structure, and approximate otherwise. In particular $F_3(X_1; X_3, X_4)$ can be thought of as the projection of $X_3 - X_1$ onto $X_4 - X_1$ (Figure \ref{fig:geom}F), or how far, along the axis from $X_1$ to $X_4$ would $X_3$ project. Similarly, $F_4(X_1, X_4; X_2, X_3)$ corresponds to the projection of $X_2 - X_3$ onto $X_1 - X_4$, and corresponds to the shorter line segment in Figure \ref{fig:geom}G. For a different configuration of the arguments, $F_4(X_1, X_2; X_3, X_4) $, we see that $X_1$ and $X_2$ project to the same spot on $X_3 - X_4$, which means that the two arguments are orthogonal Figure \ref{fig:geom}H, precisely what we want to test using an $F_4$-test \citep{patterson2012}.



 
 
 %The angle between $X_x - X_1$ and $X_x - X_2$ is obtuse if $X_2$ is in the half-plane whose boundary goes through $X_x$ and is orthogonal to $\overline{X_XX_1}$ (Figure \ref{fig:geom}C), and its value is proportional to the projection onto $X_X - X_1$. This can be useful for interpretation: if potential source pulations are related according to a tree, the admixture axis will be orthogonal to the tree branches connecting potential source populations, and the population with the most negative $F$-statistic will be the most likely source \citep{patterson2012}. 

%On the other hand, if population relationships are more gradual, the interpretation is different: The lowest $F_3$-statistics will correspond to the populations where the $n$-sphere is biggest, i.e. the ends of the cline.  This is a practical issuel for example \cite{patterson2012} observed that many Western Eurasian populations have the most negative $F_3$-statistics if one of the source population is from Europe, and one from South America, which can be thought of a gradient in relationships throughout Eurasia and the Americas. Depending on whether we think of population relationships as more clinal or tree-like, negative $F_3$-statistics will thus have very different interpretations.



\paragraph{Example}
In Figure \ref{fig:world}A, I use the World data set to visualize the outgroup-$F_3$-statistic $F_3(\text{Mbuti}; \text{Sardinian}, X_i)$, in i.e. a statistic that aims to find the population most closely related to Sardinian (a Mediterranean Island), assuming the Mbuti are an outgroup to all populations in the data set. On a PCA, we can interpret this $F_3$ statistic as the projection of the line segment from $\text{Mbuti}$ to population $X_i$ onto the line through Mbuti and Sardinians (black line). For each population, the projection is indicated with a grey line. In the full data space, this line is always orthogonal to the segment Mbuti-Sardinian, but on the plot (i.e. the subspace spanned by the first two PCs), this is not necessarily the case.  The coloring is based on the $F_3$-statistic calculated from all the data, with brighter values indicating higher $F_3$-statistics. In this case, the first two PCs approximate the $F_3$-statistic very well: Particularly the samples from East Asia and the Americas  project almost orthogonally, suggesting that most of the genetic variation relevant for this analysis is captured by these first two PCs.  We can quantify this and find that the first two PCs slightly underestimate the absolute value of $F_3$ (Figure \ref{fig:weu}C), but keep the relative ordering. I also find that many PCs, e.g. PCs 3-5, 7 and 10 have almost zero contribution to all $F_3$-statistics (Figure \ref{fig:weu}D), and PCs 6, 8 and 9 having a similar non-zero contribution for almost all statistics, likely because these PCs explain within-African variation.


\subsection{$F_4$-statistics as angles}
One interpretation of $F_4$ on PCA plots is similar to that of $F_3$; as a projection of one vector onto another, with the difference that now all four points may be distinct. $F_4$-statistics that correspond to an internal branch in a tree (as in Figure \ref{fig:geom}C), can be interpreted as being proportional to the length of a projected segment on a PCA plot (Figure \ref{fig:geom}G), again with the caveat that we need to scale it by a constant. If the $F_4$-statistic corresponds to a branch that does not exist in the tree (Figure \ref{fig:geom}D), then, from the tree-interpretation, we expect $F_4(X_1, X_2; X_3, X_4) = 0$ implies that the vectors $X_1 - X_2$ and $X_3 - X_4$ are orthogonal to each other, i.e. that $X_1$ and $X_2$  map to the same point on the projection axis $\overline{X_3X_4}$ (Figure \ref{fig:geom}H). In the case of an admixture graph, this is no longer the case: Both population $X_y$ and $X_x$ in Figure \ref{fig:admix}D do \emph{not} map to the same point as $X_1$ or $X_2$ do, implying that statistics of the form $F_4(X_1, X_x; X_3, X_4) \neq 0$.

%We can also see how this interpretation aligns with that of $F_4$ as the length of an internal branch on a tree : By assumption, disjoint sets of branches evolve independently \citep{cavalli-sforza1964, felsenstein1973, semple2003}. Since the data space is sufficiently high-dimensional, this ensures that the resulting drift trajectories will also be uncorrelated. Therefore, if we interpret $F_4(X_1, X_2; X_3, X_4)$ as the projection of $X_3 - X_4$ - onto $X_1 - X_2$, we can write  
%$$X_3 - X_4  = (X_3 - X'_3) + (X'_3 - X'_4) + (X'_4 - X_4)\text{.}$$ Of these three branches, the first and last are orthogonal to $X_1 - X_2$ and thus the $F_4$ statistic is just the internal branch of the tree (Figure \ref{fig:geom}F). It also suggests a number of diagnostic $F$-statistics that check assumptions; for example if the tree holds, then 

Since $F_4$ is a covariance, its magnitude lacks an interpretation. Therefore, commonly correlation coefficients are used, as there, zero means independence and one means maximum correlation. For $F_4$, we can write 
\begin{equation}
\text{Cor}(X_1 - X_2, X_3 - X_4) =  \frac{F_4( X_1, X_2; X_3, X_4) }{\norm{X_1-X_2}\norm{X_3-X_4}} = \cos(\phi),\label{eq:angle}
\end{equation}
where $\phi$ is the angle between $X_1 - X_2$ and $X_3 - X_4$. Thus, independent drift events lead to $\cos(\phi) = 0$, so that the angle is 90 degrees, whereas an angle close to zero means $\cos(\phi)\approx 1$, which means most of the genetic drift on this branch is shared.

\paragraph{Example}
To illustrate the angle interpretation I return to the Western Eurasian data. The PCA-biplot shows two roughly parallel clines (Figure \ref{fig:weu}A), a European gradient (from Sardinian to Finnish and Chuvash), and an Asian cline from Arab populations (top right) to the Caucasus  (bottom right). This is quantified in Figure \ref{fig:weu}D, where I plot the angle corresponding to $F_4(X, \text{Saudi}; \text{Sardinian}, \text{Finnish})$. For most Asian populations, using two PCs (green points) gives an angle close to zero, corresponding to a correlation coefficient between the two clines of $r>0.9$. Just adding a third PC (blue), however, shows that the clines are not, in fact, parallel, and the correlation for most populations is low. The finding that three PCs are necessary to explain this data can also be seen from the spectrum of these $F_4$-statistics (Figure \ref{fig:weu}E), which have high contributions from the first three PCs. Both results indicate that adding a third PC would give a much better description of the data, and the relationship between within-European variation to Saudis in particular.

\begin{figure}[!ht]
	\includegraphics[width=.6\textwidth]{figures/fig_f4_ratio.pdf}	
	\caption{\textbf{Admixture proportion estimates} A: Visualization of the admixture graph scenario used to estimate the proportion $\alpha$ contributed from $X_1$ to $X_X$, using references $R_1$ and $R_2$. The full grey line corresponds to the projection axis, and the dotted grey lines corresponds to the branches ignored in the projections. The admixture proportion $\alpha$ corresponds to the length of the dashed red line relative to the black line between $X_1$ and $X_2$. B: The same scenario, but in Euclidean space, $X_1$, $X_2$ and $X_X$ align on a line both in the (low-dimensional approximation of the) residual space and on the projection axis. 
	}	
	\label{fig:ratio}
\end{figure}

\subsection{Other projections}
So far, I used eq. \ref{eq:fpc} to interpret $F$-statistics on a PCA-plot, but the argument holds for \emph{any} orthonormal projection in the data space. This is useful in particular for estimates of admixture proportions, which are often done as projections into a low-dimensional reference space defined by $F$-statistics \citep{patterson2012, petr2019, harney2021, oteo-garcia2021}. 

For example, a common way to estimate admixture proportion $\alpha$ of $X_1$ is the $F_4$-ratio:
\begin{eqnarray}
\alpha = \frac{F_4(R_1, R_2; X_X, X_1)}{F_4(R_1, R_2; X_2, X_1)} = \frac{\vectorproj[R_1 - R_2]{X_X - X_1}}{\vectorproj[R_1 - R_2]{X_2 - X_1}} \label{eq:f4ratio},
\end{eqnarray}
which can be interpreted as projecting $X_X-X_1$ and $X_2-X_1$ onto $R_1 - R_2$ and the ratio of the lengths gives the proportion of $X_x$ contributed by $X_1$  \citep{oteo-garcia2021}. 

The admixture graph motivating this statistic is visualized in Figure \ref{fig:ratio}A, and the PCA-like interpretation in Figure \ref{fig:ratio}B. In both panels, the solid gray lines is the projection axis, and the dotted lines give the residual, i.e. the branches or genetic variation that is ignored by the projection. 

The PCA-like projection can be used to visualize admixture proportions, as the horizontal position of $X_X$ relative to $X_1$ and $X_2$ (red dashed line vs black line) directly represents the estimated admixture proportion $\alpha$. In addition, the residuals can be used to verify assumptions of the admixture graph model. In particular, since $X_X$ arises as a linear combination of $X_1$ and $X_2$, if admixture is recent we might expect the three populations to be collinear; if they are not this means that either of the populations experienced gene flow from some other population which might bias results \citep{petr2019}.


In addition, the external tree branches $X_1 - X'_1$ and $X_2 - X'_2$ are disjoint which means they should be orthogonal. On a one-dimensional residual plot (Figure \ref{fig:ratio}B) this can not be verified, but the statistic 
\begin{equation}
F_4(X_1, X'_1; X_2, X'_2) = 0
\end{equation}
can be calculated for all samples.

\subsubsection{Example}
I use the World data set as an example, using Sardinian and Mbuti as references populations (Figure \ref{fig:world}B). The data are the same as in  the PCA (Figure \ref{fig:world}A), but it is now rotated such that the axis between the reference population (black line in Figure \ref{fig:world}A) is now reflected by the x-axis. For any pair of populations $X_1$ $X_2$, their horizontal projection distance reflects $F_4(\text{Sardinian}, \text{Mbuti}; X_1, X_2)$ and the relative horizontal distance corresponds exactly to $F_4$-ratio admixture estimates. For many sets of populations, this is of course not sensible, and just looking at the first PC of the residual shows many examples where the populations are not collinear. For example, on the x-axis, the South American Surui are between Papuans and Georgians, but since the Surui clearly are not on the line between Papuans and Georgians, this cannot be the result of admixture.

\section{Discussion}
Particularly for the analysis of human genetic variation with a large number of individuals with heterogeneous relationships, $F$-statistics  are  a powerful tool to describe population genetic diversity. Here, I show that the geometry of $F$-statistics \citep{oteo-garcia2021} leads to a number of simple interpretations of $F$-statistics on a PCA-plot. 

\subsection{The Geometry of Admixture}
Previous interpretation of PCA in the context of population genetic models have focused on explicit models and aimed at directly interpreting the PCs in terms of population genetic parameters \citep{cavalli-sforza1975, novembre2008a, francois2010, francois2021}. My interpretation here is different in that the utility of PCA is to simplify the geometry of the data, rather than attributing meaning to the produced PCs. One consequence is that the results here are less directly impacted by sample ascertainment, sample sizes or number-of-PCs, which are common concerns in the interpretation of PCA \citep{novembre2008a, mcvean2009, francois2010, francois2021}; adding more PCs will provide a successively better approximation of the $F$-statistics. 

The two data sets I analyzed here suggest that two PCs (for the World data set) respectively three PCs (for the Western Eurasian data) already provide a very good approximation for $F_4$-statistics (Figures \ref{fig:weu}E, \ref{fig:world}E), reflecting the observation that frequently the first few PCs provide a good approximation of population structure. On the other hand, for admixture $F_3$-statistics, more PCs are needed (Figure \ref{fig:weu}BC), likely because these statistics do contain a term measuring local variation in the putative admixed population \citep{peter2016}, which is typically often incorporated on higher PCs. Thus, I would also expect individual-based analyses and $F_4$-statistics between closely related populations to require substantially more PCs as there local differentiation, which is typically reflected on higher PCs, becomes important.

My focus on the geometry of the data allows for direct and quantitative comparisons between $F$-statistic-based results and PCA biplots. As PCA is often ran in an early step in data analysis, this may aid in generation of hypotheses that can be more directly evaluated using generative models, typically using a lower number of populations. It also allows reconciling apparent contradictions between $F$-statistics and PCA-plots. In many cases, differences between the two data summaries will be due to variation on higher PCs. In this case, plotting additional PCs, or further subsetting the data to a more local set of populations seems prudent. 

\subsection{Assumptions}
The other cause for disagreements between $F$-statistics and PCA are differences in assumptions. The version of PCA I used for my analyses was chosen such that the similarities to $F$-statistics are maximized. In particular, I assume here that i) we have no missing data, ii) SNPs are equally weighted, and iii) that individuals can be grouped into populations and iv) we use estimated allele frequencies. In contrast, most data analyses have to grapple with missing data, SNPs are often weighted according to their allele frequencies, and observed, individual-level genotypes are used as the basis of PCA.

\subsubsection{Missing data}
The liability of PCA to missing data is a well-studied problem and a number of algorithms for imputing missing data have been proposed \citep[e.g.][]{hastie2015, meisner2018, meisner2021}. Calculating $F$-statistics first is yet another way to impute data for PCA that works well if missingness is low \citep{meisner2021}. In contrast, missing data in $F$-statistics is handled by estimating a standard error using resampling across the genome \citep{patterson2012}, which does not distinguish between biological and sampling variation. These strategies are distinct, but not unique to the relative approaches and a PCA-like decomposition from $F$-statistics is commonly applied using MDS \citep[e.g.][]{fu2016}. The theory developed here suggests that developing a PCA-based $F$-statistics framework might be more powerful, as related populations could be used to fill in missing data.

\subsubsection{Normalization}
In PCA, SNPs are typically normalized to have expected variance of one; a step that is omitted in calculating $F$-statistics \citep{patterson2006}. The $F$-statistic framework assumes that each SNP is an identically-distributed (but not independent) random variable. which holds regardless of weighting. Thus, normalization of SNPs is largely a matter of convention, for  $F$-statistics the dependency on  additional samples (through  mean allele frequencies) is often unwanted, but could be advantageous for tools that aim to do joint inference from  many $F$-statistics such as \texttt{qpAdm} \citep{patterson2012, harney2021}. As genetic differentiation between human populations is low, the  normalization used may matter little in practice, but should be explored in future work \citep{felsenstein1973}. 


\subsubsection{Estimated vs. observed allele frequencies}
The third difference between $F$-statistics and PCA is on the usage of estimated allele frequencies versus individual-based genotypes. The fact that PCA does not distinguish between sampling error and the underlying structure is a well-known drawback of PCA, and applying the theory presented here to individual-based PCA would result in $F$-statistics that incorporate some sampling noise. Probabilistic PCA is one class of approaches that aim to separate the population structure from sampling noise \citep[e.g.][]{agrawal2020}. It seems likely that probabilistic PCA would yield a representation of the data that corresponds more closely aligned with $F$-statistics than regular PCA.


\subsubsection{Individual vs. population-based analyses}
The final issue is that PCA is commonly run on individual-based data, whereas $F$-statistics often group individuals into populations. This is not necessary; population-based PCA has been the default in the past \citep{cavalli-sforza1994}, and $F$-statistics are often applied to individuals \citep[e.g.][]{green2010, massilani2020, yang2020}. Thus, more commonly the choice between individual-based and population-based approaches is in the goal of an analysis. Grouping individuals into populations introduces a major assumption, that can e.g. be justified using an individual-based PCA. In particular, since $F$-statistics assume individuals are randomly drawn from a population, they should form tight clusters on an individual-based PCA-plot, otherwise population substructure becomes a possible alternative model for negative $F_3$-statistics and non-zero $F_4$-statistics \citep{peter2016}.

\subsubsection{Summary}
The version of PCA used here differs from that proposed by \cite{patterson2006}, and thus some care will be required to directly extend the interpretations developed here to individual-based PCAs. However, the differences are largely due to conventions, and particularly for studies where the description of population structure is a major focus, results might be easier to interpret if conventions regarding missing data, normalization and estimation of allele frequencies are used consistently between $F$-statistics and PCA.

\subsection{The Apportionment of Human Diversity}
Most genetic variation in humans is shared between all of us, but the around 15\% that can be explained by population structure can be leveraged to study our history and diversity in great detail \citep{dobzhansky1972, cavalli-sforza1964, reich2018a}. For some data sets it is possible to predict an individuals' origin at a resolution of a few hundred kilometers \citep{novembre2008, leslie2015}, and direct-to-consumer-genetics companies are using this variation to analyze the genetic data of millions of customers. 

However, understanding, conceptualizing and modeling this variation is far from trivial, particularly in a historical context in which mistaken ideas about human variation have been used to justify racist, eugenic and genocidal policies. Lewontin’s landmark 1972 paper on the apportionment of human genetic diversity was one of the first to quantify how little of between-population genetic variation could be attributed to ``racial'' continental-scale groupings \citep{novembre2022}. Over the last five decades, this view has been corroborated, refined and extended many times \citep{cann1987, cavalli-sforza1994, barbujani1997, rosenberg2002a}. 

From a practical perspective, formulating hypotheses and designing studies in terms of discrete populations with ``uniform'' genetic backgrounds is often sensible, as it enables  e.g. prediction of phenotypes \citep{berg2019, yair2021}, inference of demographic parameters, and schematic models of human genetic history \citep{patterson2012}. This is also the case for $F$-statistics, which are motivated by trees. They assume that populations are discrete, related as a graph, and that gene flow between populations is rare \citep{patterson2012,harney2021}.
However, these simplifications do come at a cost, both in terms of model violations that may invalidate statistical results, and in terms of deemphasizing people that do not rigidly fall into predefined genetic groups.


In many parts of the world, and particularly at more local scales, distinctions between populations begin to blur, and everyone could be considered  admixed to some degree \citep{pickrell2014}. This provides a challenge for interpretation; as most $F_3$ and $F_4$-statistics will indicate departures from treeness. A naive interpretation of the $F$-statistics from my Eurasian example  (Figure \ref{fig:weu}A) would identify a substantial fraction of Europeans as (significantly) admixed between Finnish and Sardinians. In contrast, PCA reveals that the variation in this data set is not due to a single event, and so an arguably better description of the data set is one where Finnish and Sardinians lie on opposite ends of a more gradually structured population.

\begin{comment}
This continuum between admixture and population structure becomes more problematic when interpreting tools that  use multiple $F$-statistics to build compound models, such as \texttt{qpGraph} \citep{lazaridis2014} and \texttt{qpAdm} \citep{harney2021}. One issue with these approaches is that they are usually restricted to at most a few dozen populations. As ancient DNA data sets now commonly include thousands of individuals, analysts are faced with the challenge of which data to include. A common approach is to sample a large number of distinct models, and retain the ones that are compatible with the data. However, as both \texttt{qpGraph} and \texttt{qpAdm} assume that gene flow is rare and discrete, selecting sets of populations that did experience little gene flow will provide good fits. The PCA-based  interpretation of $F$-statistics offers an alternative that trades interpretability for robustness, as I do not need to assume that gene flow is rare.
\end{comment}

An alternative could be to focus on orthogonality of drift as a \emph{the} central concept for analyzing genetic variation data.
This is often informally done in the context of PCA, where it is often observed that a particular PC is associated with variation within a specific region, or separates out distinct populations \citep{cavalli-sforza1994}. These associations are not exact, and impacted e.g. by the size and composition of the analyzed data set. On the other hand, we can use $F$-statistics to formally test orthogonality. Distinct clades in an admixture graph would be one possibility how orthogonality could arise, but assuming a graph is not necessary. Motivated by PCA, we could set up $F_4$ as tests of orthogonality due to either space (distinct populations evolve independently) or scale (i.e. between-population diversity is independent of within-population diversity). This slight generalization of $F$-statistics could allow us to reframe many of the questions that are currently asked as tests of orthogonality without assuming discrete lineages.

\section{Conclusion}
$F$-statistics and PCA have both proven to be tremendously useful to study, visualize and test aspects of population structure. Here, I show that these approaches are closely connected, and highlight a few implications. First, PCA and $F$-statistics should never be treated as independent analyses. If they agree, this can be used as a sanity check that no major assumptions about e.g. population groupings are violated, but the underlying biological relationships investigated are the same in both approaches.  

If sufficient PCs are considered, both $F_3$ and $F_4$-statistics do have simple interpretations in PCA-space. If used as a test for admixture, $F_3$ corresponds to a $n$-sphere in PCA-space, which manifests as a circle on a PCA-plot. The informal notion that an admixed population should lie between its sources on a PCA-plot is necessary, but not sufficient; unadmixed populations may also project there. In contrast, it is sufficient (but not necessary) to determine that a population has a positive $F_3$ statistic. 

Interpretations of the outgroup-$F_3$-statistics and $F_4$-statistics in a PCA-framework rely heavily on the idea of a projection: In a tree or admixture-graph framework, we can loosely interpret $F_4(A, B; C, D)$ as a projection of one the path $A-B$ onto $C-D$; only edges shared between the paths will contribute non-zero amounts to this statistics. This projection analogy holds when we replace the populations by points in PCA-space, which enables us to study population models beyond discrete graphs. In this context, $F_4$ is simply an orthogonality test, an $F_4$ of zero means that the pairs of populations are orthogonal (i.e. independent) of another. 

One difficulty in interpreting the results here is that they only hold when all PCs are considered. Yet, in my applications I found that two (for the World data set) and three PCs (for Western Eurasia) already provided a very good approximation for the majority of $F$-statistics, so that the results here should be applicable to at least continental-scale data sets.


\section{Supplementary File Description}
Both supplementary files give the individual-identifier, sex and population label for each sample.

%\bibliography{main}
\printbibliography

\newpage
\appendix
\section{Derivations}\label{appendix:fonpc}
\setcounter{equation}{0}
\renewcommand{\theequation}{\thesection\arabic{equation}}
Depending on a readers' background in linear algebra, these results may appear elementary; I include them here for reference and because they were not obvious to me at the onset of this project.
\paragraph{$F$-statistics are invariant under a change-of-basis}
\begin{eqnarray}
F_2(X_i, X_j) &=& \sum_{l=1}^S \big( (x_{il} - \mu_l) -(x_{jl} -\mu_l)\big)^2 = F_2(Y_i, Y_j)\nonumber\\
&=& \sum_{l=1}^S \big( \sum_k L_{kl}P_{ik} - \sum_kL_{kl}P_{jk}\big)^2\nonumber\\
&=& \sum_{l=1}^S \left( \sum_k L_{kl} (P_{ik} -P_{jk}) \right)^2\nonumber\\
&=& \sum_{l=1}^S \left( \sum_k L_{kl}^2 (P_{ik} -P_{jk})^2 + 2\sum_{k\neq k'} L_{kl}L_{k'l}(P_{ik} - P_{jk'})^2 \right)\nonumber\\
&=& \sum_k \underbrace{\left(\sum_{l=1}^L L_{kl}^2\right)}_1 (P_{ik} -P_{jk})^2 + 2\sum_{k\neq k'}\underbrace{\left(\sum_{l=1}^S L_{kl}L_{k'l}\right)}_{0} (P_{ik} - P_{jk'})^2\nonumber\\
&=& \sum_k (P_{ik} - P_{jk})^2 \label{eq:changeofbasis}
\end{eqnarray}

In summary, the first row shows that $F_2$ on the centered data will give the same results (as distances are invariant to translations), in the second row we apply the PC-decomposition. The third row is obtained from factoring out $L_{lk}$. Row four is obtained by multiplying out the sum inside the square term for a particular $l$. We have $k$ terms when for $\binom{k}{2}$ terms for different $k$'s.  Row five is obtained by expanding the outer sum and grouping terms by $k$.The final line is obtained by recognizing that $\ML$ is an orthonormal basis; where dot products of different vectors have lengths zero.

Note that if we estimate $F_2$, unbiased estimators are obtained by subtracting the population-heterozygosities $H_i, H_j$ from the statistic. As these are scalars, they do not change above calculation.

\paragraph{The region of negative $F_3$-statistics is a $n$-ball}
Without loss of generality, assume that $X_1 = (r, 0, 0, \dots)$ and $X_2 = (-r, 0, 0, \dots)$, and let us assume that $X_x$ has coordinates $(x_1, x_2, \dots, x_S)$ Assuming $F_3(X_x; X_1, X_2) = 0$, equation \ref{eq:f3neg} becomes
\begin{align}
2 F_3(X_x; X_1, X_2) &=  \normsq{X_x - X_1} + \normsq{X_x - X_2}  - \normsq{X_1 - X_2} = 0 \nonumber\\
&= \left[(x_1-r)^2 + \sum_{i=2}^S x_i^2\right] + \left[(x_1 + r)^2 + \sum_{i=2}^S x_i^2\right] - 4r^2 \nonumber\\
&= 2 \left[\sum_{i=1}^S x_i^2 + r^2 + x_1r - x_1r\right] - 4r^2\nonumber\\
F_3(X_x; X_1, X_2) & = -r^2 + \sum_{i=1}^S x_i^2 = -r^2 + \normsq{X_x} = 0  \text{,} \label{eq:f3sphere}
\end{align}
which is the equation of a $n$-sphere with radius $r$ and center at the origin, as assumed from the placing of $X_1$ and $X_2$. Now, assume that $F_3$ is negative, i.e. $F_3(X_x; X_1, X_2) = -k < 0$. Moving $r^2$ to the left we obtain
\begin{equation}
r^2 - k = \normsq{X_x} \text{,}
\end{equation}
which is another $n$-sphere with a smaller radius, showing that all points inside the $n$-sphere will have negative $F_3$-values.

\paragraph{If a population lies outside the circle of this $n$-Sphere in any 2D-projection, $F_3$ is positive} 
Assume the center of the $n$-sphere $C = \frac{X_1 + X_2}{2} = (c_1, c_2, \dots c_S)$, and $X_x = (x_1, x_2, \dots x_S)$.  Then, 
\begin{align}
F_3(X_x; X_1, X_2) &= \normsq{X_x - C} - r^2 \nonumber\\
&= \underbrace{(x_1 - c_1)^2 + (x_2 - c_2)^2}_{>r^2} + \underbrace{\sum_{i=3}^S (x_i - c_i)^2}_{\geq 0} - r^2 \nonumber\\
& > 0 \text{.}\label{eq:f3circle}
\end{align}
The condition $(x_1 - c_1)^2 + (x_2 - c_2)^2 >r^2$ is satisfied whenever $X_x$ is outside the circle obtained from projecting the $n$-sphere on the first two dimensions. An analogous argument applies for any low-dimensional representation.


\end{document}
