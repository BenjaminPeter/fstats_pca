\documentclass[12pt,a4pape, fullpage]{article}
\usepackage[
margin=1.5cm,
includefoot,
footskip=30pt,
]{geometry}
\usepackage[utf8x]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{natbib}
\usepackage{lineno}
\usepackage{url}
\usepackage{textcomp}

\bibliographystyle{evolution.bst}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normsq}[1]{\left\lVert#1\right\rVert^2}


\newcommand{\MX}{\mathbf{X}} %uncentered data
\newcommand{\MC}{\mathbf{C}} %centering
\newcommand{\MY}{\mathbf{Y}} %centered data
\newcommand{\MF}{\mathbf{F}_2} %F2-distance matrix
\newcommand{\MFT}{\mathbf{F}_3} %F3-distance matrix
\newcommand{\MP}{\mathbf{P}} % PCs
\newcommand{\ML}{\mathbf{L}} % loadings
\newcommand{\MK}{\mathbf{K}} % Kernel
\newcommand{\MSINGULAR}{\mathbf{\Sigma}} % Singular values matrix
\newcommand{\MEIGEN}{\mathbf{\Lambda}} % Eigenvalue matrix
\newcommand{\MEAN}{\boldsymbol{\mu}} % Kernel


\begin{document}
\section{Introduction}
About 15\% of genetic variation in humans can be explained by population structure \citep{dobzhansky1972, barbujani1997, rosenberg2002}, but the information contained in these 15\% is sufficient to study our genetic diversity and history in great detail \citep{cavalli-sforza1994,reich2018a}. For some data sets it is possible to predict an individuals origin at a resolution of a few hundred kilometers \citep{novembre2008, leslie2015}, and direct-to-consumer-genetics companies are using this variation to analyze the genetic data of millions of customers.

\begin{comment}
Understanding and characterizing this variation is also crucial for genomic medicine: Risk loci identified in one population perform progressively worse in more distant populations \citep{berg2019, duncan2019}; so identifying a good ``reference'' data set is crucial. Understanding genetic variation is also important when designing studies, in order to measure which populations studies should target \citep{fullerton2016}. Despite these needs, no generally accepted model for human population structure exists. The historical view that humans diversity can be partitioned into  distinct ``races'' \citep[e.g.][]{blumenback1798} has long been superseded in favor of more accurate and nuanced models. These models fail because they only explain a small amount of variation...

In Lewontin's pioneering analysis, he found that less than half (6\%), of that variation could be attributed to the continental-scale groups he called races, it seemed  which he used to claim that ''racial classification is (...) seen to be of virtually no genetic or taxonomic significance``.
\end{comment}

Human genetic diversity has both discrete and continuous components \citep{rosenberg2002, serre2004, rosenberg2005, bradburd2018, reich2018a}. On the one hand, barriers to migration such as mountain ranges, oceans, or simply large distances reduce gene flow \citep{bradburd2013, peter2020a, rosenberg2005} and cause genetic differentiation. On the other hand, major population movements such as the out-of-Africa, Austronesian or Bantu expansions and local gene flow lead to more gradual patterns of genetic diversity \cite{cavalli-sforza1994, ramachandran2005, novembre2008, peter2020a, stoneking2018, racimo2020}, sometimes including partial replacement and gene flow with resident populations. Finally, long-distance migrations \citep{alves2015} and secondary contact and admixture between diverged populations, such as Neandertals and early modern humans \citep{green2010} add another layer.

This complex population structure is frequently handled by using multiple models with different assumptions; each emphasizing a particular aspects of the data. Data-driven methods such as Principal Component Analysis (PCA) \citep{cavalli-sforza1994} structure \citep{pritchard2000} or multidimensional scaling (MDS, \cite{malaspinas2014}) are often used to display the full complexity of the data, but they have the disadvantage that they are not easily interpretable. For this purpose, more explicit demographic models \citep{gutenkunst2009, kamm2015, excoffier2013} are applied, which allow for parameter estimation or hypothesis tests. 

Particularly in the analysis of human ancient DNA, a set of techniques based on $F$-statistics \textit{sensu} Patterson have risen in popularity \citep{patterson2012, peter2016}. This framework is based on the assumption that the relationship between three or four populations is often tree-like, and allows for a variety of tests of treeness and more complex demographic models \citep{patterson2012, harney2021}. However, the connections between PCA, $F$-statistics and demographic models are currently unclear, which makes quantitative comparisons, detecting model violations and joint interpretation of the results of these approaches difficult. Since both $F$-statistics and PCA are functions of expected pairwise coalescent times \citep{mcvean2009, peter2016}, this is one avenue to link these approaches. Here, I instead use the geometric interpretation of $F$-statistics introduced by  \cite{oteo-garcia2021} to directly visualize $F$-statsitics on a PCA plot. 


	
\section{Theory}
In this section, I will give a very brief formal introduction to $F$-statistics and PCA. A more detailed technical treatise of PCA is given in e.g. \cite{jolliffe2013}, and a useful guide to interpretation is \cite{cavalli-sforza1994}. Readers unfamiliar with $F$-statistics may find \cite{patterson2012}, \cite{peter2020a} or \cite{oteo-garcia2021} helpful.

\subsection{Introduction to PCA}
Let us assume we have some genotype data summarized in a matrix $\MX$ whose entry $x_{ij}$ reflects the allele frequency of the $i$-th population at the $j$-th genotype. If we have $S$ SNPs and $n$ populations, $\MX$ will have dimension $n \times S$. As a population may be represented by just one individual, there is no conceptual difference between these cases and I will
refer to populations as unit for analysis. Since the allele
frequencies are between zero and one, we can interpret each Population $X_i$
of $\MX$ as a point in $[0, 1]^S$, the allele frequency or \emph{data space}.
	
The goal of PCA is to find a low-dimensional subspace $\mathbb{R}^K$ that explains most of the variation in the data. $K$ is at most $n-1$, in which case the data is simply rotated. However, the  historical processes that generated genetic variation often result in \emph{sparse} data \citep{engelhardt2010}, so that $K \ll n$ explains a substantial portion of the variation; for visualization $K=2$ is frequently used. 
	
\begin{comment}
	\begin{figure}
		\includegraphics[width=\textwidth]{pca_explanation.png}
		\caption{Basic Idea of PCA from 2D to 1D representation. A: Allele frequencies from different populations (blue dots) at two SNPs. A PCA is performed by centering the data (B), and rotating it (B) such that the first PC explains the majority of variation in the data, and the second PC is orthognal to the first, and explains the residual. A lower-dimensional approximation (in this case 1D) can be achieved by just keeping the first PC (E); which can be translated back to the original data space by inverting the rotation and centering (F).}
		\label{fig:pca_explanation}
	\end{figure}
\end{comment}
	
There are several algorithms that are used to perform PCAs, the most common one is based on singular value decomposition \citep{jolliffe2013}. In this approach, we first mean-center $\MX$, obtaining a centered matrix $\MY$
	\begin{equation*}
	y_{il} = x_{il} - \mu_l
	\end{equation*}
	where $\mu_l$ is the mean allele frequency at the $l$-th locus.
	
	PCA can then be written as
	
	\begin{equation}
	\MY = \MC\MX = (\mathbf{U} \MSINGULAR) \mathbf{V}^T = \MP\ML\text{,}
	\end{equation}
	
	where $\MC = \mathbf{I} -\frac{1}{n}\mathbf{1}$ is a centering matrix that subtracts row means, with $\mathbf{I}, \mathbf{1}$  the identity matrix and a matrix of ones, respectively. The orthogonal matrix of principal components $\MP=\mathbf{U}\MSINGULAR$ has size $n \times n$ and is used to reveal population structure. The SNP loadings $\ML=\mathbf{V}^T$ are an orthonormal matrix of size $n \times k$, its rows give the contribution of each SNP to each PC, it is often useful to look for outliers that might be indicative of selection \citep[e.g][]{francois2010}.
	
	In many  implementations \citep[e.g.][]{patterson2006}, SNPs are weighted by
    the inverse of their standard deviation. As this weighting often makes little
    difference in practice \citep{mcvean2009}, I will assume throughout that SNPs
    are unweighted.

\subsection{Introduction to $F$-statistics}
PCA is typically used to model population structure between many populations. $F$-statistics take the opposite approach, revealing the relationship between just two,  three or four populations at a time. The three $F$-statistics can be defined as 
\begin{subequations}
	\begin{align}
	&F_2(X_1, X_2) &=& \sum_{l=1}^S(x_{1l} - x_{2l})^2 &=& \normsq{X_1, X_2}\\
	&F_3(X_1; X_2, X_3) &=& \sum_{l=1}^S(x_{1l} - x_{2l})(x_{1l} - x_{3l}) &=& \langle X_1 - X_2, X_1 - X_3 \rangle\\	
	&F_4(X_1, X_2; X_3, X_4) &=& \sum_{l=1}^S(x_{1l} - x_{2l})(x_{3l} - x_{4l}) &=& \langle X_1 - X_2, X_3 - X_4 \rangle	\text{,}
	\end{align}
\end{subequations}
where $\norm{\cdot}$ denotes the Euclidean norm and $\langle \cdot, \cdot \rangle$ denotes the dot product. Furthermore, both $F_3$ and $F_4$ can be written as sums of $F_2$-statistics:
\begin{subequations}
	\begin{align}
2F_3(X_1; X_2, X_3) &= 2F_2(X_2, X_3) - F_2(X_1, X_2) + F_2(X_1, X_3)\\
2F_4(X_1, X_2; X_3, X_4) &= F_2(X_1, X_3) + F_2(X_2, X_4) - F_2(X_1,X_4) - F_2(X_2, X_3)
	\end{align}
\end{subequations}

$F$-statistics have been primarily motivated in the context of trees and admixture graphs \citep{patterson2012}. In a tree, the squared Euclidean distance $F_2(X, Y)$ measures the length of all branches between populations $X$ and $Y$; and $F_3$ and $F_4$ represent external and internal branches in a tree, respectively \citep{peter2016}. The length is a measure of genetic drift, and is non-negative if data is generated under a tree \citep{patterson2012}. This interpretation is useful to understand a number of applications. The outgroup-$F_3$-statistic $F_3(O; U, X_i)$, for example, is useful if we have an unknown population $U$, and want to find its closest relatives from a panel of populations $X_i$. The highest values of $F_3$ indicate the population $X_i$ most closely related to $U$, using the outgroup $O$ to correct for differences in sample times. The population $X_i$ with the largest value is the most closely related population out of the reference sample. The internal branches described by  $F_4$-statistics are frequently used for complex models, such as  reconstructing admixture graphs \citep{patterson2012, lipson2013} and estimating admixture proportions \citep{petr2019, harney2021}.

Most commonly however, $F_3$ and $F_4$ are used as admixture tests \citep{patterson2012}: Negative values of  $F_3(X_1; X_2, X_3) < 0$ correspond to a branch with negative genetic drift, which is a violation of treeness. Similarly if four populations are related as a tree, then at least one of the $F_4$ statistics between the populations will be zero \citep{patterson2012}.

To move away from trees and graph models, I build upon the geometric framework of \cite{oteo-garcia2021}. Here, we think of each population as a point in the data space $\mathbb{R}^S$, made up of the allele frequency at each SNP. Then, $F_2(X_1, X_2) = \normsq{X_1 - X_2}$ is the squared Euclidean distance between two populations $X_1$ and $X_2$, and $F_4(X_1, X_2 ; X_3, X_4) = \langle X_1 - X_2, X_3 - X_4 \rangle$ is the inner (dot) product between these two vectors. These dot products are useful for a variety of projections that use population structure.



\subsection{Connection between PCA and $F$-statsitics}	
\subsubsection{Principal components from $F$-statistics}
PCA and $F$-statistics are closely related. In fact, the principal components can be directly calculated from $F$-statistics using multidimensional scaling \citep{gower1966}. Suppose we calculate the pairwise $F_2(X_i, X_j)$ between all $n$ populations, and collect them in a matrix $\MF$. We can obtain the principal components from this matrix by double-centering it, so that its row and column means are zero, and perform an eigendecomposition of the resulting matrix:
\begin{equation}
\MP\MP^T = - \frac{1}{2}\MC\MF\MC \text{.} \label{eq:mds}
\end{equation}

\subsubsection{$F$-statistics in PCA-space}
By performing a PCA, we rotate our data to reveal the axes of highest variation. However, the dot product is invariant under rotation, and $F$-statistics can be thought of as dot products.  What this means is that we are free to calculate $F_2$ either on the uncentered data $\MX$, the centered data $\MY$ or any other orthogonal basis such as the principal components $\MP$. Formally,

\begin{align}
F_2(X_i, X_j) &=&  \sum_{l=1}^L \big( x_{il} -x_{jl}\big)^2  &&\nonumber\\ 
 &=& \sum_{l=1}^L \big( (x_{il} - \mu_l) -(x_{jl} -\mu_l)\big)^2   &=& F_2(Y_i, Y_j) \nonumber\\
 &=& \sum_{k=1}^n (p_{ik} - p_{jk})^2  &=& F_2(P_i, P_j) \text{,}
\end{align}
A detailed derivation of this is given in Appendix \ref{appendix:fonpc}.
As $F_3$ and $F_4$ can be written as sums of $F_2$-terms, analogous relations apply.

In most applications, we do not use all PCs, but instead use only the first $K$ PCs.
Thus, 
\begin{equation}
F_2(P_i, P_j) = \underbrace{\sum_{k=1}^K(p_{ik} - p_{jk})^2}_{\hat{F_2}^{(K)}(P_i, P_j)} + \sum_{k=K+1}^n(p_{ik} - p_{jk})^2 \text{.}
\end{equation}
If we sum up the approximation errors $F_2 - \hat F_2$ over all pairs of populations, we obtain the Frobenius-norm of the error $\normsq{\MF - \hat{\MF}}_F$; which is precisely the function that is minimized in MDS \citep{jolliffe2013}. Thus, $\hat\MF$ is the optimal approximation of $\MF$ for any $K$.


\subsection{Geometry of $F$-statistics in PC-space}
The transformation from the previous section allows us to consider the geometry
of $F$-statistics in PCA-space. The relationships we will discuss formally only
hold if we use all $n-1$ PCs. However, the appeal of PCA is that frequently,
only a very small number $K \ll n$ of PCS contain most information that is
relevant for population structure (for visualization
$K=2$ is often used).

\subsubsection{$F_2$ in PC-space}
The $F_2$-statistic is an estimate of the squared Euclidean distance between two
populations. It thus corresponds to the squared distance in PCA-space, and
reflects that closely related populations  will be close to each other on a
PCA-plot, and have low pairwise $F_2$-statistics. In converse, if two
populations with high $F_2$ lie on the same point on an PCA-plot, this suggests
that substantial variation is hidden on higher PCs.



\subsubsection{When is $F_3$ negative?}
The $F_3$-statistic becomes more interesting; as outlines above we either think of $F_3$ as ``outgroup''-$F$-stats or as admixture $F$-stats. In the admixture case, we may ask the following question: given two source populations $X_1$, $X_2$, where would admixed populations on a PCA plot lie? From theory, we would expect it to lie between $X_1$ and $X_2$, with the exact location depending on sample sizes \citep{brisbin2012, mcvean2009}. 

Formally, we would reject admixture if $F_3$ is negative, i.e. we are looking for the space
\begin{eqnarray}
2 F_3(X_x; X_1, X_2) &=& 2\langle  X_x - X_1, X_x - X_2 \rangle \nonumber\\
      &=& \normsq{X_x - X_1} + \normsq{X_x - X_2}  - \normsq{X_1 - X_2} 
\end{eqnarray}
By the Pythagorean theorem, $F_3 = 0 $ iff $X_1, X_2$ and $X_x$ form a right-angled triangle. In a 2D-PCA plot, the region where $F_3$ is zero is the circle  with diameter $\overline{X_1X_2}$, and if $X_x$ lies inside this circle, $F_3(X_x; X_1, X_2) < 0$. If the 

ball, the angle is obtuse and $F_3$ is negative, otherwise it will be positive. If we approximate the PCA-space in two dimensions, the $n$-ball corresponds to a circle. 

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{figures/fstats_on_pca.pdf}
	\caption{\textbf{Geometric representation of $F$-statistics on 2D-PCA-plot.} A: $F_2$ represents the squared Euclidean distance between two points in PC-space. B: Admixture-$F_3(X_x; X_1, X_2)$ is negative if $X_x$ lies in the circle specified by the diameter $X_2-X_1$}. C: $F_3(X_x; X_1, X_2)$ is negative given $X_1, X_x$ if $X_2$ is in the gray space.  D: Outgroup-$F_3$ reflects the projection of $X_2 - X_O$ on $X_1 - X_O$. E: $F_4$ is the projection of $X_3 - X_4$ on $X_1-X_2$. F: If $X_x$ is admixed between $X_1$ and $X_2$, the admixture proportions will be projected.
\end{figure}

\subsubsection{$F_4$ and right angles}
The inner-product-interpretation of $F_4$ is similar to that of $F_3$, with the change that the two vectors we consider do not involve the same population. However, a finding of $F_4(X_1, X_2; X_3, X_4) = \langle X_1 - X_2, X_3 - X4 \rangle = 0$ similarly implies that the two vectors are orthogonal, and a non-zero value reflects the projection of one vector on the other.

\subsubsection{$F_4$-ratio}
\begin{eqnarray}
\frac{F_4(X_I, X_O; X_X, X_1)}{F_4(X_I, X_O; X_2, X_1)} &=& \frac{\norm{X_I-X_O}\norm{X_X-X_1}\cos(\alpha)}{\norm{X_I-X_O}\norm{X_2-X_1}\cos(\beta)}\nonumber\\
&=&\frac{\norm{X_X-X_1}\cos(\alpha)}{\norm{X_2-X_1}\cos(\beta)}\nonumber\\
&=& \frac{\norm{X_X' - X_1'}}{\norm{X_2' - X_1'}}
\end{eqnarray}
where $\alpha$ and $\beta$ are the angles between vectors, and $X_i'$ is the projection of $X_i$ on $X_I-X_O$.

Conjecture: Thus, we are measuring the distances between the admixing populations on the projected on the axis between $X_I$ and $X_O$. This ought to be valid only if $\langle X_1 - X_1', X_2 - X_2' \rangle$ are orthogonal to each other, and to $X_OX_I$, i.e.
$F_4(X_1, X_1', X_2, X_2') = 0$
 
	

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{figures/fig_f3_data.pdf}
	\caption{\textbf{PCA and $F_3$-statistics} A: PCA of Western Eurasian data; the circle denotes the region for which $F_3(X; \text{Basque}, \text{Turkish})$ may be negative. Populations for which $F_3$ is negative are colored in red. B, E: $F_3$ approximated with two (blue) and ten (red) PCs versus the full spectrum. C,F: Contributions of PCs 1-10 to each $F_3$-statistic. D: PCA of World data set, color indicates value of $F_3(\text{Mbuti}; \text{Mozabite}, X)$. The black line shows the projection axis Mbuti-Mozabite, the gray lines indicates the projected position of each population. }
	\label{fig:f3}
\end{figure}


	

\section{Results}
The theory outlined in the previous section suggests that $F$-statistics have a geometric interpretation on PCA plots, and can be approximated using PCA. I illustrate this on two human data sets based on the ``Human Origins''-SNP set (597,573 SNPs). Both are based on the analyses if \citep{lazaridis2014}, and I use subsets of the Reich lab compendium data set (v44.3), downloaded from \url{https://reich.hms.harvard.edu/allen-ancient-dna-resource-aadr-downloadable-genotypes-present-day-and-ancient-dna-data}. 

\paragraph{World Overview data set}
This data set represents global human genetic variation (638 individuals from 33 population), as used by \citep{lazaridis2014}. As this data set is very sparse, it may be well-approximated by an admixture graph.

\paragraph{West-Eurasian data set}
This data set of 1,119 individuals from 62 populations contains  present-day individuals from the Eastern Mediterranean, Caucasus and Europe. It is frequently used as a basis of comparison for ancient genetic analyses of Western Eurasian individuals \citep{patterson2012, lazaridis2014}. Genetic differentiation in this region is low and closely mirrors geography \citep{novembre2008}, and thus not particularly graph-like.

I perform analyses at the level of populations to ease presentation.  It is an assumption of $F$-statistics that the genetic variation within sampled population is independent of the variation between samples. I use \texttt{admixtools 2.0.0} \url{https://github.com/uqrmaie1/admixtools} to compute a matrix of $F_2$-statistics between all populations. To obtain a PC-decomposition I use equation \ref{eq:mds} and the \texttt{eigen} function in \texttt{R}, and compare them with the $F_3$ and $F_4$-statistics calculated using admixtools 2.

\paragraph{Admixture-$F_3$}
I visualize the admixture statistic $F_3(X; Basque, Turkish)$, on the first two PCs of the Westeurasian data set (Figure \ref{fig:f3}A). 
%This PCA presents two parallel clines, one from the Levant and Arabia (``BedouinB'') to the Caucasus (``Abkhasian''), and a second one from Southern (``Sardinian'') to Northeastern Europe (``Mordovian''). 
This statistic aims to model In this context, I examine  i.e. a statistic that aims to ask which populations can be represented as a mixture between a Southwestern (Basque) and Southeastern (Turkish) European population. The -- largely Southern European -- populations for which the point estimate of these $F_3$-statistic is negative are highlighted in red. They both fall close to the center of the $F_3$-circle, either defined on the first two (dark grey) or all PCs (light gray). However, many populations inside the circle on the first 2 PCs, including English, Sardinians and Canary Islanders have positive $F_3$-values, on higher PCs, showing that the first two PCs do not capture all the genetic variation related to population structure for this data set. 

This is expected because for spatially continuous populations, PCA will not be sparse \citep{novembre2008a}.  Consequently, approximating $F_3$ by the first two or ten PCs (Figure \ref{fig:f3}B) only gives a coarse approximation of $F_3$, and from Figure \ref{fig:f3}C we see that many higher PCs contribute to $F_3$ statistics.

Thus, the main benefit of this PCA-plot is that it allows us to identify populations outside the circle (from the Levant and Caucasus), for which $F_3$ is guaranteed to be positive.

\paragraph{Outgroup-$F_3$}
The Outgroup-$F_3$-statistic is commonly used to infer which population is closest in a set of reference populations. In Figure \ref{fig:f3}D, I present a PCA of the world data set, with populations colored according to $F_3(Mbuti; Mozabite, X_i)$, i.e. a statistic that is commonly interpreted as finding the population $X_i$ that is most closely related to Mozabite. On a PCA, we can interpret this $F_3$ statistic as the projection of the line segment $\text{Mbuti}X_i$ onto the line through Mbuti and Mozabite (black line). For each population, the projection is indicated with a grey line. In the full data space, this line is always orthogonal to the segment Mbuti-Mozabite, but on the plot (i.e.) the subspace spanned by the first two PCs, this is only true if the relevant variation is captured by the first two PCs. We see that particularly the samples from East Asia, Siberia and the Americas project very close to orthogonally, suggesting that most of the variation is captured by these first two PCs. That the approximation of $F_3$ on 


\subsection{$F_4$}
Using $F_3$-statistics, I showed that we can think of the admixture test as a test of whether the admixed population lies in a particular $n$-ball, and the outgroup $F_3$-statistic can be thought of as a projection of the test populations on the line connecting the outgroup to the reference sample. In this section, I will develop similar interpretation of $F_4$ on PCA-plots, and to investigate sparsity. 

First, we investigate the sparsity in the world overview data set: We find that the vast amount of contribution to the statistics comes from the first two PCs (Figure \ref{fig:f4}A). For example, the correlation between $F_4(X, Y, \text{Mozabite}, \text{Yoruba})$ and its approximation using the first two PCs is 99.2\%. To visualize the interpretation of $F_4$ as an angle, we use statistics of the form $F_4(X, \text{Sardinian}; \text{Mozabite}, \text{Yoruba})$, which can be interpreted as the angle between the vectors Mozabite-Yoruba and $X$-Sardinian. In Figure \ref{fig:f4}B, I show the angle based on two (blue), ten (green) and all PCs $red$. I find that for most Asian and American populations the angle is very close to 90°, as would be expected if the variation between African and non-African populations is mostly orthogonal. On the other hand, if $X$ is an African population, the angle is lower, and much less well approximated. This demonstrates that this PCA-plot likely does not model within-African population structure adequately. 

The $F_4$-statistics for the Westeurasian data set are slightly less sparse, the correlation coefficients between $F_4(X, \text{French}; \text{Finnish}, \text{Canary Islander})$ and its approximation using the first two or three PCs is 95.5\% and 99.1\% respectively (Figure \ref{fig:f4}E). I also show that the interpretation of $F_4$ as a projection can be used as a useful visualization (Figure \ref{fig:f4}D). On the $x$-axis, I plot $\langle X; \text{Finnish}, \text{Canary Islander}\rangle$, so that the horizontal distance between all pairs of populations corresponds to their respective $F_4$-statistics $F_4(X, Y; \text{Finnish}, \text{Canary Islander})$. On the $Y$-axis and with the coloring I display the first two principal components of the residual, i.e. the genetic variation that is missed by viewing the data through this projection. We find that most European populations have positive values on residual PC1, and are relatively closely clustered. In contrast Middle Eastern and Caucasian populations have negative values on this gradient. This allows us to visualize that this particular $F_4$-projection does an adequate job if we are interested in describing European variation, but it fails to explain the non-European data. We can further quantify this by investigating the percent of variance explained on each axis (Figure \ref{fig:f4}F), where I find that the projection axis only describes around 12\% of hte variation, compared to residual PC1 with almost 30\%.



\begin{figure}[!ht]
\includegraphics[width=\textwidth]{figures/fig_f4_data.pdf}	
	\caption{\textbf{PCA and $F_4$-statistics} A: Spectrum of select $F_4$-statistics in World data set. B: Projection angle representation of $F_4(X, Sardinian; Mozabite, Yoruba)$ (red) and approximations using two (blue) and ten (green) PCs. C: Spectrum of select $F-4$-statistics in Westeurasian data set. D: Scatterplot of $F_4$-projection on Finnish-Canary Islanders axis and residual PC1.
	E: $F_4(X, \text{French}, \text{Finnish}, \text{Canary Islander})$ vs. prediction using two (blue) and ten(red) PCs. F: Percent variance explained for the projection of panel D and the first nine residual PCs.
	}
\label{fig:f4}
\end{figure}

\section{Discussion}
Particularly for the analysis of ancient DNA, $F$-statistics have been established as a powerful tool to describe population genetic diversity, but they have a number of limitations. In particular, they assume that populations are discrete, related as a graph, and that gene flow between populations is rare \citep{patterson2012,harney2021}. As a consequence, researchers concerned about model fits may ascertain reference populations in a way that satisfies these assumptions, thus inadvertently making population structure appear sparser than it truly is. This is perhaps most obvious from Figure \ref{fig:f3}B, where large gaps are present. However, these gaps are due to sparse sampling that disappear if more populations were sampled \citep[e.g.][]{peter2020a}, not due to gaps in genetic diversity. If population ascertainment is not done very carefully, tools built on top of $F$-statistics, such as \texttt{qpGraph} and \texttt{qpAdm}, may thus only provide a very loose lower bound for the number of gene flow events.

In contrast, the perspective on $F$-statistics in data space \citep{oteo-garcia2021} and on PCA does not require assumptions on number of admixture or gene flow events. Independent of any model, A population $X_x$ can be thought of as admixed between $ X_1$ and $X_2$ if it lies in the ball with diameter $\overline{X_1X_2}$. And independent of any models, two axes of variation that are not perpendicular to each other shows that there is some degree of shared history. 

The connection between the graph-based and PCA-based interpretations of $F_3$ and $F_4$ is due to concept of projection. As illustrated above, both $F$-statistics can be thought of as projections on any particular axis of variation. If that variation corresponds to a tree-branch, the interpretations align. A collorary to this interpretation is the importance of orthogonality, or independence for describing gene flow. Populations without gene-flow will evolve independently, and their changes in allele frequency will therefore be orthogonal, resulting in $F_4$-statistics of zero, and right angles on a PCA plot. Even though this is only true when considering all PCs, the result holds reasonably well when just looking at the first two PCs -- in our example of the world-overview data, I found that the variation in Sub-Saharan Africa is mostly -- but not completely -- uncorrelated with the variation between European and Asian populations (Figure \ref{fig:f4}A).

While the data space produces a much larger model space than trees, the main drawback of the PCA-based interpretation is a lack of interpretability; it is not easy to define a generative model that could generate a complex PCA-plot. Thus, in the future the tree- and PCA-based interpretations will likely be used in conjunction. 

 However, despite the apparent complexity, the PC-spaces \emph{are} sparse, and using just ten or even two PCs often gives very good approximations.






To make PCA and $F$-statistics more comparable in practical settings, there are a number of -- mainly statistical -- concerns that still need to be addressed in future work. The perhaps most obvious one is that PCA is most frequently run on individuals, whereas $F$-statistics are often calculated on populations. This is not a conceptual issue, as both PCA and $F$-statistics can be run on either \citep{cavalli-sforza1994}. Population based analyses have the advantage that they are easier to interpret and compute (current packages are ill-equipped to calculate all pairwise $F$-statistics between data sets with thousands of individuals \citep{patterson2012}). However, this requires the assumption that the within-population variation is independent from the between-population variation; something that is analogous to the variance partitioning based on PCs here. 

A second difference is that frequently, rare SNPs are weighted higher in PCA, whereas all SNPs are weighted the same for $F$-statistics \citep{patterson2006}. This is only a difference of convention; $F$-statistics could also be calculated using the same weighting. The close connection between the two approaches developed here suggest that for most analyses, users might want to be consistent and use the same weighting for both types of analyses. 

The third and perhaps biggest gap are statistical issues. The treatment here focusses on the mean estimated $F$-statistic, but many  applications of $F$-statistics are based on hypothesis tests \citep{patterson2012}. This requires estimating accurate standard errors for these statistics, which is difficult since nearby SNPs will be correlated \citep{hahn2018}. In contrast, standard PCA does not modeljointly models the covariance matrix due to population structure and sampling. On the other hand, for both data sets I investigated here, the matrix $\MF$ of $F$-statistics estimated using admixtools2 is not a proper squared Euclidean distance matrix, i.e. it is not negative semidefinite and has imaginary PCs. This is not a practical when considering single $F$-statistics or PCA (for analyses here, I used a nearby matrix  \citep{higham2002} with no apparent loss of precision). It does however mean that tools that use matrices of $F$-statistics, such as \texttt{qpadm} or \texttt{qpgraph} may be ill-calibrated, which may partly explain why they generally have poor out-of-sample predictive power and are restricted to a few dozen samples at a time. A model-based framework based on probabilistic PCA \citep{meisner2021, agrawal2020, hastie2015} would likely be able to generate consistent $F$-statistics and PCs, while incorporating sampling error and missing data.

\begin{itemize}
    \item weighting of SNPs
    \item estimation error
    \item propagating erros
    \item exploratory data analysis
    \item missing data
    \item population vs. sample allele frequencies
\end{itemize}

\appendix
\section{Derivation}\label{appendix:fonpc}
\begin{eqnarray}
F_2(X_i, X_j) &=& \sum_{l=1}^L \big( (x_{il} - \mu_l) -(x_{jl} -\mu_l)\big)^2 = F_2(Y_i, Y_j)\nonumber\\
&=& \sum_{l=1}^L \big( \sum_k L_{kl}P_{ik} - \sum_kL_{kl}P_{kj}\big)^2\nonumber\\
&=& \sum_{l=1}^L \left( \sum_k L_{kl} (P_{ik} -P_{jk}) \right)^2\nonumber\\
&=& \sum_{l=1}^L \left( \sum_k L_{kl}^2 (P_{ik} -P_{jk})^2 + 2\sum_{k\neq k'} L_{kl}L_{k'l}(P_{ik} - P_{jk'})^2 \right)\nonumber\\
&=& \sum_k \underbrace{\left(\sum_{l=1}^L L_{kl}^2\right)}_1 (P_{ik} -P_{jk})^2 + \sum_{k\neq k'}\underbrace{\left(\sum_{l=1}^L L_{kl}L_{k'l}\right)}_{0} (P_{ik} - P_{jk'})^2\nonumber\\
&=& \sum_k (P_{ik} - P_{jk})^2
\end{eqnarray}

In summary, the first row shows that $F_2$ on the centered data will give the same results (as distances are invariant to translations), in the second row we apply the PC-decomposition. The third row is obtained from factoring out $L_{lk}$. Row four is obtained by multiplying out the sum inside the square term for a particular $l$. We have $k$ terms when for $\binom{k}{2}$ terms for different $k$'s.  Row five is obtained by expanding the outer sum and grouping terms by $k$.The final line is obtained by recognizing that $\ML$ is an orthonormal basis; where dot products of different vectors have lengths zero.

Note that if we estimate $F_2$, unbiased estimators are obtained by subtracting the population-heterozygosities $H_i, H_j$ from the statistic. As these are scalars, they do not change above calculation.
\bibliography{main}

\end{document}
